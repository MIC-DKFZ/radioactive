{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import einops as E\n",
    "import random\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "sys.path.append('/home/t722s/Desktop/UniversalModels/OtherRepos/UniverSeg')\n",
    "from universeg import universeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tensors(tensors, col_wrap=8, col_names=None, title=None):\n",
    "    M = len(tensors)\n",
    "    N = len(next(iter(tensors.values())))\n",
    "    \n",
    "    cols = col_wrap\n",
    "    rows = math.ceil(N/cols) * M\n",
    "\n",
    "    d = 2.5\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(d*cols, d*rows))\n",
    "    if rows == 1:\n",
    "      axes = axes.reshape(1, cols)\n",
    "    \n",
    "    for g, (grp, tensors) in enumerate(tensors.items()):\n",
    "        for k, tensor in enumerate(tensors):\n",
    "            col = k % cols\n",
    "            row = g + M*(k//cols)\n",
    "            x = tensor.detach().cpu().numpy().squeeze()\n",
    "            ax = axes[row,col]\n",
    "            if len(x.shape) == 2:\n",
    "                ax.imshow(x,vmin=0, vmax=1, cmap='gray', aspect = 'equal')\n",
    "            else:\n",
    "                ax.imshow(E.rearrange(x,'C H W -> H W C'))\n",
    "            if col == 0:\n",
    "                ax.set_ylabel(grp, fontsize=16)\n",
    "            if col_names is not None and row == 0:\n",
    "                ax.set_title(col_names[col])\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            ax = axes[i,j]\n",
    "            ax.grid(False)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    \n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=20)\n",
    "            \n",
    "    plt.tight_layout()\n",
    "\n",
    "def get_slices(volume_path, fg_label = None, size = (128,128)):\n",
    "    '''\n",
    "    Given a path to a nifti volume, obtain a list of slices, resizing them to the given resolution and return slices as a list of tensors\n",
    "    '''\n",
    "    volume = nib.load(volume_path).get_fdata()\n",
    "    if fg_label: # If is a label, set all other entries to 0. Mostly useful for multisegmentation datasets. \n",
    "        volume[volume!= fg_label] = 0\n",
    "\n",
    "    volume = torch.from_numpy(volume).float() # Universeg requries floats, not doubles.\n",
    "    slices = torch.unbind(volume, dim = 2)\n",
    "\n",
    "    # Resize slices if desired:\n",
    "    if size:\n",
    "        resize_factor = np.array(size)/np.array(slices[0].shape)\n",
    "        order = 0 if fg_label else 1 # nearest neighbour interpolation for labels, bilinear for images\n",
    "\n",
    "        slices = [torch.from_numpy(zoom(slice, resize_factor, order = order))\n",
    "                        for slice in slices]\n",
    "\n",
    "    return(slices)\n",
    "\n",
    "def get_slices_resize3D(volume_path, sizeXY = (128,128)):\n",
    "    '''\n",
    "    As with get_slices, but rescales volumes rather than each slice. I thought this'd be quicker, but it seems to be slower and I'm not sure why.\n",
    "    '''\n",
    "    volume = nib.load(volume_path).get_fdata()\n",
    "\n",
    "    # resize volume if desired\n",
    "    if sizeXY:\n",
    "        size = sizeXY + (volume.shape[2],)\n",
    "        resize_factor = np.array(size)/np.array(volume.shape)\n",
    "        volume = zoom(volume, resize_factor, order = 1)\n",
    "\n",
    "    volume = torch.from_numpy(volume).float() # Universeg requries floats, not doubles.\n",
    "    slices = torch.unbind(volume, dim = 2)\n",
    "\n",
    "\n",
    "    return(slices)\n",
    "\n",
    "def classify_list(l, delta = 3):\n",
    "    '''\n",
    "    Given a list of 1s and 0s (ie slices with foreground, slices with no foreground within a volume), return list of equal length classifying each element as 'bg', 'bgb', 'fgb' and 'fg', meaning background, background border, foreground border\n",
    "    and foreground, with 'background border' meaning a 0 within delta of a 1, and similarly for 'foreground border'.\n",
    "    '''\n",
    "\n",
    "    n = len(l)\n",
    "    classification = [''] * n\n",
    "\n",
    "    for i in range(n):\n",
    "        # Check if within delta indices, there's an element of the other type\n",
    "        lower = max(i-delta, 0)\n",
    "        upper = min(i+delta+1, n)\n",
    "        \n",
    "        is_border = any(l[j] != l[i] \n",
    "                        for j in range(lower, upper))\n",
    "\n",
    "        if l[i] == 1:\n",
    "            classification[i] = 'fgb' if is_border else 'fg'\n",
    "        else:\n",
    "            classification[i] = 'bgb' if is_border else 'bg'\n",
    "\n",
    "    return classification  \n",
    "\n",
    "def get_support_inds(l, num_of_slices = [1,1,1,1]): \n",
    "    '''\n",
    "    Given a list l classified as by classify_list (distinguishing between foreground etc as 'bg', 'bgb', 'fgb', 'fg'), gets uniformly sampled indices for each class to use as indices for support slices. The number per class is defind by\n",
    "    num_of_slices, representing the desired number of support slices from background, background border, foreground border and foreground respectively\n",
    "    '''\n",
    "\n",
    "    support_inds = []\n",
    "\n",
    "    for i, cls in enumerate(['bg', 'bgb', 'fgb', 'fg']):\n",
    "        n = num_of_slices[i]\n",
    "\n",
    "        # Obtain sample n indices uniformly at random of type cls from l \n",
    "        cls_inds = [i for i, x in enumerate(l)  if x == cls] # ie the slice indices of type cls\n",
    "\n",
    "        if n > l.count(cls):\n",
    "            raise RuntimeError(f'Not enough slices of type {cls}')\n",
    "        \n",
    "        sampled_inds = random.sample(cls_inds, n)\n",
    "\n",
    "        support_inds.extend(sampled_inds)\n",
    "\n",
    "    return(support_inds)\n",
    "\n",
    "def get_support_slices_labels(support_volume_paths, support_label_paths, fg_label, device):# Include infer label option for binary tasks\n",
    "    '''\n",
    "    The paths in support_volume_paths and support_label_paths must correspond. Returns (support_slices, support_labels).\n",
    "    '''\n",
    "\n",
    "    if len(support_volume_paths) != len(support_label_paths):\n",
    "        raise RuntimeError(f'Unequal number of support volume paths and labels given, {len(support_volume_paths)}, {len(support_label_paths)} respectively')\n",
    "    \n",
    "    support_slices = []\n",
    "    support_labels = []\n",
    "\n",
    "    # Obtain support slices and their labels\n",
    "    for i in range(len(support_volume_paths)):\n",
    "        # Load in volumes and labels as tuples of tensors\n",
    "        volume_slices = get_slices(support_volume_paths[i])\n",
    "        label_slices = get_slices(support_label_paths[i], fg_label = fg_label)\n",
    "\n",
    "        # Choose indices to use to select support slices and labels\n",
    "        fg_present = [int(fg_label in slice) for slice in label_slices] \n",
    "        fg_present_nuanced = classify_list(fg_present)\n",
    "        support_inds = get_support_inds(fg_present_nuanced)\n",
    "\n",
    "        # obtain support slices and labels from support volumes and labels\n",
    "        support_slices.extend([volume_slices[i][None] for i in support_inds])\n",
    "        support_labels.extend([label_slices[i][None] for i in support_inds])\n",
    "\n",
    "    support_slices = torch.stack(support_slices).to(device)\n",
    "    support_labels = torch.stack(support_labels).to(device)\n",
    "    return (support_slices, support_labels)\n",
    "\n",
    "def dice_score(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    y_pred = y_pred.long()\n",
    "    y_true = y_true.long()\n",
    "    score = 2*(y_pred*y_true).sum() / (y_pred.sum() + y_true.sum())\n",
    "    return score.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(model, image, label, support_images, support_labels, device = 'cuda'):\n",
    "    image, label = image.to(device), label.to(device)\n",
    "\n",
    "    # inference\n",
    "    logits = model(\n",
    "        image[None],\n",
    "        support_images[None],\n",
    "        support_labels[None]\n",
    "    )[0] # outputs are logits        \n",
    "\n",
    "    soft_pred = torch.sigmoid(logits)\n",
    "    hard_pred = soft_pred.round().clip(0,1)\n",
    "\n",
    "    #  score\n",
    "    score = dice_score(hard_pred, label)\n",
    "\n",
    "    # return a dictionary of all relevant variables\n",
    "    return {'Image': image,\n",
    "            'Soft Prediction': soft_pred,\n",
    "            'Prediction': hard_pred,\n",
    "            'Ground Truth': label,\n",
    "            'score': score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/t722s/Desktop/Datasets/amosForUniversegTest/labelsTr/amos_0001.nii.gz',\n",
       " '/home/t722s/Desktop/Datasets/amosForUniversegTest/labelsTr/amos_0004.nii.gz',\n",
       " '/home/t722s/Desktop/Datasets/amosForUniversegTest/labelsTr/amos_0005.nii.gz',\n",
       " '/home/t722s/Desktop/Datasets/amosForUniversegTest/labelsTr/amos_0006.nii.gz',\n",
       " '/home/t722s/Desktop/Datasets/amosForUniversegTest/labelsTr/amos_0007.nii.gz',\n",
       " '/home/t722s/Desktop/Datasets/amosForUniversegTest/labelsTr/amos_0008.nii.gz',\n",
       " '/home/t722s/Desktop/Datasets/amosForUniversegTest/labelsTr/amos_0009.nii.gz',\n",
       " '/home/t722s/Desktop/Datasets/amosForUniversegTest/labelsTr/amos_0010.nii.gz']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_label_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fg_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 2\u001b[0m label_slices \u001b[38;5;241m=\u001b[39m get_slices(\u001b[43msupport_label_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m fg_present \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(fg_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mslice\u001b[39m \u001b[38;5;129;01min\u001b[39;00m label_slices] \n\u001b[1;32m      4\u001b[0m fg_present_nuanced \u001b[38;5;241m=\u001b[39m classify_list(fg_present)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "fg_label = 2\n",
    "label_slices = get_slices(support_label_paths[8])\n",
    "fg_present = [int(fg_label in slice) for slice in label_slices] \n",
    "fg_present_nuanced = classify_list(fg_present)\n",
    "fg_present_nuanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurable parameters \n",
    "device = 'cuda'\n",
    "\n",
    "# Obtain support slices\n",
    "volume_dir = '/home/t722s/Desktop/Datasets/amosForUniversegTest/imagesTr/'\n",
    "volume_paths = sorted([os.path.join(volume_dir, file) for file in os.listdir(volume_dir)]) # in later usage, if a random selection is desired, don't use sorted here, and use np.random.permutation later\n",
    "\n",
    "label_dir = '/home/t722s/Desktop/Datasets/amosForUniversegTest/labelsTr/'\n",
    "label_paths = sorted([os.path.join(label_dir, file) for file in os.listdir(label_dir)])\n",
    "# Split volumes, labels into support and query sets\n",
    "# volume_paths, label_paths = np.random.permutation(volume_paths), np.random.permutation(label_paths)\n",
    "n_support_volumes = 8\n",
    "support_volume_paths,   query_volume_paths  = volume_paths[:n_support_volumes], volume_paths[n_support_volumes:]\n",
    "support_label_paths,    query_label_paths   = label_paths[:n_support_volumes],  volume_paths[n_support_volumes:] \n",
    "\n",
    "support_slices = []\n",
    "support_labels = []\n",
    "\n",
    "# Obtain support slices and their labels\n",
    "for i in range(len(support_volume_paths)):\n",
    "    # Load in volumes and labels as tuples of tensors\n",
    "    volume_slices = get_slices(support_volume_paths[i])\n",
    "    label_slices = get_slices(support_label_paths[i], fg_label = 1)\n",
    "\n",
    "    # Choose indices to use to select support slices and labels\n",
    "    fg_present = [int(fg_label in slice) for slice in label_slices] \n",
    "    fg_present_nuanced = classify_list(fg_present)\n",
    "    support_inds = get_support_inds(fg_present_nuanced)\n",
    "\n",
    "    # obtain support slices and labels from support volumes and labels\n",
    "    support_slices.extend([volume_slices[i][None] for i in support_inds])\n",
    "    support_labels.extend([label_slices[i][None] for i in support_inds])\n",
    "\n",
    "support_slices = torch.stack(support_slices).to(device)\n",
    "support_labels = torch.stack(support_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Not enough slices of type bgb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m support_volume_paths,   query_volume_paths  \u001b[38;5;241m=\u001b[39m volume_paths[:n_support_volumes], volume_paths[n_support_volumes:]\n\u001b[1;32m     14\u001b[0m support_label_paths,    query_label_paths   \u001b[38;5;241m=\u001b[39m label_paths[:n_support_volumes],  volume_paths[n_support_volumes:] \n\u001b[0;32m---> 17\u001b[0m support_slices, support_labels \u001b[38;5;241m=\u001b[39m \u001b[43mget_support_slices_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43msupport_volume_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msupport_label_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfg_label\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m visualize_tensors({\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupport images\u001b[39m\u001b[38;5;124m'\u001b[39m: support_slices[:\u001b[38;5;241m8\u001b[39m],\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupport labels\u001b[39m\u001b[38;5;124m'\u001b[39m: support_labels[:\u001b[38;5;241m8\u001b[39m]\n\u001b[1;32m     23\u001b[0m })\n",
      "Cell \u001b[0;32mIn[80], line 146\u001b[0m, in \u001b[0;36mget_support_slices_labels\u001b[0;34m(support_volume_paths, support_label_paths, fg_label, device)\u001b[0m\n\u001b[1;32m    144\u001b[0m fg_present \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(fg_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mslice\u001b[39m \u001b[38;5;129;01min\u001b[39;00m label_slices] \n\u001b[1;32m    145\u001b[0m fg_present_nuanced \u001b[38;5;241m=\u001b[39m classify_list(fg_present)\n\u001b[0;32m--> 146\u001b[0m support_inds \u001b[38;5;241m=\u001b[39m \u001b[43mget_support_inds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfg_present_nuanced\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# obtain support slices and labels from support volumes and labels\u001b[39;00m\n\u001b[1;32m    149\u001b[0m support_slices\u001b[38;5;241m.\u001b[39mextend([volume_slices[i][\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m support_inds])\n",
      "Cell \u001b[0;32mIn[80], line 118\u001b[0m, in \u001b[0;36mget_support_inds\u001b[0;34m(l, num_of_slices)\u001b[0m\n\u001b[1;32m    115\u001b[0m cls_inds \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(l)  \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mcls\u001b[39m] \u001b[38;5;66;03m# ie the slice indices of type cls\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m l\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot enough slices of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    120\u001b[0m sampled_inds \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(cls_inds, n)\n\u001b[1;32m    122\u001b[0m support_inds\u001b[38;5;241m.\u001b[39mextend(sampled_inds)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Not enough slices of type bgb"
     ]
    }
   ],
   "source": [
    "# Configurable parameters \n",
    "device = 'cuda'\n",
    "\n",
    "# Obtain support slices\n",
    "volume_dir = '/home/t722s/Desktop/Datasets/amosForUniversegTest/imagesTr/'\n",
    "volume_paths = sorted([os.path.join(volume_dir, file) for file in os.listdir(volume_dir)]) # in later usage, if a random selection is desired, don't use sorted here, and use np.random.permutation later\n",
    "\n",
    "label_dir = '/home/t722s/Desktop/Datasets/amosForUniversegTest/labelsTr/'\n",
    "label_paths = sorted([os.path.join(label_dir, file) for file in os.listdir(label_dir)])\n",
    "# Split volumes, labels into support and query sets\n",
    "# volume_paths, label_paths = np.random.permutation(volume_paths), np.random.permutation(label_paths)\n",
    "n_support_volumes = 8\n",
    "support_volume_paths,   query_volume_paths  = volume_paths[:n_support_volumes], volume_paths[n_support_volumes:]\n",
    "support_label_paths,    query_label_paths   = label_paths[:n_support_volumes],  volume_paths[n_support_volumes:] \n",
    "\n",
    "\n",
    "support_slices, support_labels = get_support_slices_labels(support_volume_paths, support_label_paths, fg_label = 2, device = device)\n",
    "\n",
    "\n",
    "visualize_tensors({\n",
    "    'support images': support_slices[:8],\n",
    "    'support labels': support_labels[:8]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference:\n",
    "model = universeg(pretrained=True).to(device)\n",
    "\n",
    "# load in query volume and label\n",
    "i = 0\n",
    "query_volume_path = support_volume_paths\n",
    "query_label_path = 'HaN_OAR/9/label.nii.gz'\n",
    "\n",
    "query_volume_slices = get_slices(query_volume_path)\n",
    "query_label_slices = get_slices(query_label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 20\n",
    "slice = query_volume_slices[idx][None].to(device) # Include None to add 1 dimensional color channel\n",
    "label = query_label_slices[idx][None].to(device)\n",
    "\n",
    "logits = model(slice[None], support_slices[None], support_labels[None])[0].to('cpu')\n",
    "pred = torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x250 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = {'data': [slice, label, pred, pred > 0.5]}\n",
    "titles = col_names=['slice', 'label', 'pred (soft)', 'pred (hard)']\n",
    "visualize_tensors(res, col_wrap=4, col_names=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dkfz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
