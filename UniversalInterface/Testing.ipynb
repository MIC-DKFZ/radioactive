{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import json\n",
    "import torchio as tio\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/t722s/Desktop/Datasets/BratsMini/imagesTs/BraTS2021_01646.nii.gz'\n",
    "label_path = '/home/t722s/Desktop/Datasets/BratsMini/labelsTs/BraTS2021_01646.nii.gz'\n",
    "prompts_path = '/home/t722s/Desktop/Sam-Med3DTest/BratsMini/prompts.pkl'\n",
    "metadata_path = '/home/t722s/Desktop/Datasets/BratsMini/dataset.json'\n",
    "\n",
    "with open(prompts_path, 'rb') as f:\n",
    "    prompts_dict = pickle.load(f)\n",
    "\n",
    "with open(metadata_path, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "img_name = os.path.basename(img_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = sitk.GetArrayFromImage(sitk.ReadImage(img_path))\n",
    "gt = sitk.GetArrayFromImage(sitk.ReadImage(label_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## comparison\n",
    "def binarize(gt, organ, metadata): # TODO: Raise errors for metadata not having labels, labels in the wrong k,v order, and organ not being present\n",
    "    organ_label = int(metadata['labels'][organ])\n",
    "    gt_binary = torch.where(gt == organ_label, 1, torch.zeros_like(gt))\n",
    "    return(gt_binary)\n",
    "\n",
    "\n",
    "class SegmenterInputs():\n",
    "    def __init__(self, img_path, dim, prompt, cropped_shape = None, crop_pad_center = None, xyz = True, standardize = True,): # xyz: if prompts and images are in xyz: need to be converted to zyx\n",
    "        self.img_path = img_path\n",
    "        self.img_name = os.path.basename(self.img_path)\n",
    "        self.prompt = prompt\n",
    "        self.crop_pad_center = crop_pad_center\n",
    "        self.img = sitk.GetArrayFromImage(sitk.ReadImage(img_path))\n",
    "        self.prompt_types = prompt.keys()\n",
    "\n",
    "        # Points generated from generate_points.py are in xyz since the image is read in through sitk.GetArrayFromImage on sitk.ReadImage output, and here self.img is also in xyz. Transform both to zyx\n",
    "        if xyz:\n",
    "            self.img = self.img.transpose(2,1,0)\n",
    "            if 'points' in self.prompt_types:\n",
    "                self.prompt['points'] = (self.prompt['points'][0][:,::-1], self.prompt['points'][1])\n",
    "\n",
    "        if isinstance(cropped_shape, int):\n",
    "            cropped_shape = (cropped_shape, cropped_shape, cropped_shape)\n",
    "\n",
    "        if crop_pad_center == 'from_points': \n",
    "            if 'points' not in self.prompt_types:\n",
    "                return RuntimeError('Inferring the crop/pad center from points was requested, but no points were supplied')\n",
    "            \n",
    "            bbox_min = self.prompt['points'][0].T.min(axis = 1) # Get an array of two points: the minimal and maximal vertices of the minimal cube parallel to the axes bounding the points\n",
    "            bbox_max = self.prompt['points'][0].T.max(axis = 1) + 1 # Add 1 since we'll be using this for indexing # TESTING: Remove 'self's here\n",
    "\n",
    "            point_center = np.mean((bbox_min, bbox_max), axis = 0)\n",
    "\n",
    "            self.crop_pad_center = point_center\n",
    "\n",
    "        if self.crop_pad_center is not None:\n",
    "            self.crop_pad_params = self._get_crop_pad_params(self.img, self.crop_pad_center, cropped_shape)\n",
    "            # Crop/pad image\n",
    "            self.img = self._crop(self.img, self.crop_pad_params[0])\n",
    "            self.img = self._pad(self.img, self.crop_pad_params[1])\n",
    "\n",
    "            # Crop/pad prompts\n",
    "            if 'points' in self.prompt_types:\n",
    "                self.points_transformed = self._transform_points(self.prompt['points'][0], *self.crop_pad_params)\n",
    "                #self.points_transformed = self._transform_points(self.prompt['points'][0], self.crop_pad_params[1],self.crop_pad_params[0])\n",
    "\n",
    "        if standardize == True:\n",
    "            mask = self.img > 0\n",
    "            mean, std = self.img[mask].mean(), self.img[mask].std()\n",
    "            # standardize_func = tio.ZNormalization(masking_method=lambda x: x > 0)\n",
    "            # self.img2 = np.array(standardize_func(torch.from_numpy(self.img).unsqueeze(0))).squeeze(0) # Gives a different result??\n",
    "            self.img = (self.img-mean)/std \n",
    "\n",
    "    def _crop(self, image, cropping_params): # Modified from TorchIO cropOrPad\n",
    "        low = cropping_params[::2]\n",
    "        high = cropping_params[1::2]\n",
    "        index_ini = low\n",
    "        index_fin = np.array(image.shape) - high \n",
    "        i0, j0, k0 = index_ini\n",
    "        i1, j1, k1 = index_fin\n",
    "        image_cropped = image[i0:i1, j0:j1, k0:k1]\n",
    "\n",
    "        return(image_cropped)\n",
    "\n",
    "    def _pad(self, image, padding_params): # Modified from TorchIO cropOrPad\n",
    "        paddings = padding_params[:2], padding_params[2:4], padding_params[4:]\n",
    "        image_padded = np.pad(image, paddings, mode = 'constant', constant_values = 0)  \n",
    "\n",
    "        return(image_padded)\n",
    "\n",
    "    def _get_crop_pad_params(self, img, crop_pad_center, target_shape): # Modified from TorchIO cropOrPad\n",
    "        subject_shape = img.shape\n",
    "        padding = []\n",
    "        cropping = []\n",
    "\n",
    "        for dim in range(3):\n",
    "            target_dim = target_shape[dim]\n",
    "            center_dim = crop_pad_center[dim]\n",
    "            subject_dim = subject_shape[dim]\n",
    "\n",
    "            center_on_index = not (center_dim % 1)\n",
    "            target_even = not (target_dim % 2)\n",
    "\n",
    "            # Approximation when the center cannot be computed exactly\n",
    "            # The output will be off by half a voxel, but this is just an\n",
    "            # implementation detail\n",
    "            if target_even ^ center_on_index:\n",
    "                center_dim -= 0.5\n",
    "\n",
    "            begin = center_dim - target_dim / 2\n",
    "            if begin >= 0:\n",
    "                crop_ini = begin\n",
    "                pad_ini = 0\n",
    "            else:\n",
    "                crop_ini = 0\n",
    "                pad_ini = -begin\n",
    "\n",
    "            end = center_dim + target_dim / 2\n",
    "            if end <= subject_dim:\n",
    "                crop_fin = subject_dim - end\n",
    "                pad_fin = 0\n",
    "            else:\n",
    "                crop_fin = 0\n",
    "                pad_fin = end - subject_dim\n",
    "\n",
    "            padding.extend([pad_ini, pad_fin])\n",
    "            cropping.extend([crop_ini, crop_fin])\n",
    "\n",
    "        padding_params = np.asarray(padding, dtype=int)\n",
    "        cropping_params = np.asarray(cropping, dtype=int)\n",
    "\n",
    "        return cropping_params, padding_params  # type: ignore[return-value]\n",
    "\n",
    "    def _transform_points(self, pts, cropping_params, padding_params):\n",
    "\n",
    "        # if type(pts) is torch.Tensor:\n",
    "        #     pts = pts.numpy()\n",
    "\n",
    "        axis_add, axis_sub = padding_params[::2], cropping_params[::2] \n",
    "        pts = pts + axis_add - axis_sub # same as pts_trans[:,i] = pts_trans[:,i] + axis_add[i] - axis_sub[i] iterating over i\n",
    "\n",
    "        return(pts)\n",
    "    \n",
    "\n",
    "class Points():\n",
    "    def __init__(self, points_list, points_labels):\n",
    "        self.pts = points_list\n",
    "        self.labs = points_labels\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join([f'{pt}: {\"fg\" if lab else \"bg\"}' for pt, lab in zip(self.pts, self.labs)])\n",
    "    \n",
    "class Box():\n",
    "    def __init__(self, min_coord, max_coord):\n",
    "        self.min_coord = min_coord\n",
    "        self.max_coord = max_coord\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join(f'min coord: {self.min_coord}\\n max coord: {self.max_coord}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = prompts_dict[img_name][1]['3D']\n",
    "t = SegmenterInputs(img_path, 3, {'points': pts}, 128, 'from_points')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universalModels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
