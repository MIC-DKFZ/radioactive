{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchio as tio\n",
    "import torch\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crop(image, cropping_params):\n",
    "\n",
    "    low = cropping_params[::2]\n",
    "    high = cropping_params[1::2]\n",
    "    index_ini = low\n",
    "    index_fin = np.array(image.shape) - high \n",
    "    i0, j0, k0 = index_ini\n",
    "    i1, j1, k1 = index_fin\n",
    "    image_cropped = image[i0:i1, j0:j1, k0:k1]\n",
    "\n",
    "    return(image_cropped)\n",
    "\n",
    "def _pad(image, padding_params):\n",
    "    paddings = padding_params[:2], padding_params[2:4], padding_params[4:]\n",
    "    image_padded = np.pad(image, paddings, mode = 'constant', constant_values = 0)  \n",
    "\n",
    "    return(image_padded)\n",
    "\n",
    "def invertCropOrPad(image, padding_params, cropping_params):\n",
    "    if padding_params is not None:\n",
    "        image = _crop(image, padding_params)\n",
    "    if cropping_params is not None:\n",
    "        image = _pad(image, cropping_params)\n",
    "\n",
    "    return(image)\n",
    "\n",
    "def getCroppingParams(subject, mask_name, target_shape):\n",
    "    '''Function to get the cropping and padding parameters used in an apply_transform call of torchio.CropOrPad, which can then be used to invert the transformation later on'''\n",
    "\n",
    "    mask_data = subject[mask_name].data.bool().numpy()\n",
    "\n",
    "    subject_shape = subject.spatial_shape\n",
    "    bb_min, bb_max = _bbox_mask(mask_data[0])\n",
    "    center_mask = np.mean((bb_min, bb_max), axis=0)\n",
    "    padding = []\n",
    "    cropping = []\n",
    "\n",
    "    for dim in range(3):\n",
    "        target_dim = target_shape[dim]\n",
    "        center_dim = center_mask[dim]\n",
    "        subject_dim = subject_shape[dim]\n",
    "\n",
    "        center_on_index = not (center_dim % 1)\n",
    "        target_even = not (target_dim % 2)\n",
    "\n",
    "        # Approximation when the center cannot be computed exactly\n",
    "        # The output will be off by half a voxel, but this is just an\n",
    "        # implementation detail\n",
    "        if target_even ^ center_on_index:\n",
    "            center_dim -= 0.5\n",
    "\n",
    "        begin = center_dim - target_dim / 2\n",
    "        if begin >= 0:\n",
    "            crop_ini = begin\n",
    "            pad_ini = 0\n",
    "        else:\n",
    "            crop_ini = 0\n",
    "            pad_ini = -begin\n",
    "\n",
    "        end = center_dim + target_dim / 2\n",
    "        if end <= subject_dim:\n",
    "            crop_fin = subject_dim - end\n",
    "            pad_fin = 0\n",
    "        else:\n",
    "            crop_fin = 0\n",
    "            pad_fin = end - subject_dim\n",
    "\n",
    "        padding.extend([pad_ini, pad_fin])\n",
    "        cropping.extend([crop_ini, crop_fin])\n",
    "    \n",
    "    # Conversion for SimpleITK compatibility\n",
    "    padding_array = np.asarray(padding, dtype=int)\n",
    "    cropping_array = np.asarray(cropping, dtype=int)\n",
    "    if padding_array.any():\n",
    "        padding_params = tuple(padding_array.tolist())\n",
    "    else:\n",
    "        padding_params = None\n",
    "    if cropping_array.any():\n",
    "        cropping_params = tuple(cropping_array.tolist())\n",
    "    else:\n",
    "        cropping_params = None\n",
    "    return padding_params, cropping_params  # type: ignore[return-value]\n",
    "\n",
    "def _bbox_mask(mask_volume: np.ndarray):\n",
    "        \"\"\"Return 6 coordinates of a 3D bounding box from a given mask.\n",
    "\n",
    "        Taken from `this SO question <https://stackoverflow.com/questions/31400769/bounding-box-of-numpy-array>`_.\n",
    "\n",
    "        Args:\n",
    "            mask_volume: 3D NumPy array.\n",
    "        \"\"\"  # noqa: B950\n",
    "        i_any = np.any(mask_volume, axis=(1, 2))\n",
    "        j_any = np.any(mask_volume, axis=(0, 2))\n",
    "        k_any = np.any(mask_volume, axis=(0, 1))\n",
    "        i_min, i_max = np.where(i_any)[0][[0, -1]]\n",
    "        j_min, j_max = np.where(j_any)[0][[0, -1]]\n",
    "        k_min, k_max = np.where(k_any)[0][[0, -1]]\n",
    "        bb_min = np.array([i_min, j_min, k_min])\n",
    "        bb_max = np.array([i_max, j_max, k_max]) + 1\n",
    "        return bb_min, bb_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING: points_list pre crop: [[70 48 59]\n",
    " [65 37 52]\n",
    " [64 63 73]\n",
    " [68 47 75]\n",
    " [67 91 62]]\n",
    "TESTING: pad/crop ((1, 0, 0, 0, 2, 22), (0, 33, 1, 31, 0, 0))\n",
    "TESTING: points list post crop: tensor([[76, 46, 70],\n",
    "        [53, 36, 67],\n",
    "        [74, 62, 66],\n",
    "        [60, 47, 72],\n",
    "        [63, 90, 69]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128, 128])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pts = np.array(([[70, 48, 59],\n",
    " [65, 37, 52],\n",
    " [64, 63, 73],\n",
    " [68, 47, 75],\n",
    " [67, 91, 62]]))\n",
    "points_mask = np.zeros((104,160,160))\n",
    "points_mask[*pts.T] = 1\n",
    "\n",
    "image = sitk.GetArrayFromImage(sitk.ReadImage('/home/t722s/Desktop/Datasets/BratsMini/imagesTs/BraTS2021_01646.nii.gz'))\n",
    "label = sitk.GetArrayFromImage(sitk.ReadImage('/home/t722s/Desktop/Datasets/BratsMini/labelsTs/BraTS2021_01646.nii.gz'))\n",
    "\n",
    "subject = tio.Subject(\n",
    "    image = tio.ScalarImage(tensor = torch.from_numpy(image).permute(2,1,0).unsqueeze(0)), # add channel dimension to everything, and permute to x,y,z orientation\n",
    "    points_mask = tio.LabelMap(tensor = torch.from_numpy(points_mask).permute(2,1,0).float().unsqueeze(0)),\n",
    "    label = tio.LabelMap(tensor = torch.from_numpy(label).permute(2,1,0).unsqueeze(0))\n",
    ")\n",
    "\n",
    "padding_params, cropping_params = getCroppingParams(subject, 'points_mask', [128,128,128])\n",
    "\n",
    "t = tio.CropOrPad((128,128,128), mask_name = 'points_mask')\n",
    "subject = t(subject)\n",
    "subject.image.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = points_mask.transpose(2,1,0)\n",
    "mask_crop = invertCropOrPad(mask, cropping_params, padding_params)\n",
    "\n",
    "mask_uncrop = invertCropOrPad(mask_crop, padding_params, cropping_params)\n",
    "mask_uncrop = mask_uncrop.transpose(2,1,0)\n",
    "#print(np.argwhere(mask_uncrop), pts)\n",
    "\n",
    "# The invert function works: The pts are the same after inverting as they were before applying the transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0, 0, 27) (0, 32, 0, 32, 3, 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[59, 48, 67],\n",
       "        [52, 37, 62],\n",
       "        [73, 63, 61],\n",
       "        [75, 47, 65],\n",
       "        [62, 91, 64]]),\n",
       " array([[52, 37, 62],\n",
       "        [59, 48, 67],\n",
       "        [62, 91, 64],\n",
       "        [73, 63, 61],\n",
       "        [75, 47, 65]]),\n",
       " tensor([[ 0, 52, 37, 62],\n",
       "         [ 0, 59, 48, 67],\n",
       "         [ 0, 62, 91, 64],\n",
       "         [ 0, 73, 63, 61],\n",
       "         [ 0, 75, 47, 65]]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(padding_params, cropping_params)\n",
    "pts_turned = pts[:,::-1]\n",
    "pts_turned_trans = pts_turned.copy()\n",
    "pts_turned_trans[:,2]-=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6],\n",
       "       [ 4,  8],\n",
       "       [ 6, 10]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming pts_trans, axis_add, and axis_sub are defined\n",
    "# For example:\n",
    "pts_trans = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "axis_add = np.array([1, 5])\n",
    "axis_sub = np.array([0, 1])\n",
    "\n",
    "# Perform the operation in a single line\n",
    "pts_trans += axis_add - axis_sub\n",
    "pts_trans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([5,6])\n",
    "type(a) == torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_trans = torch.argwhere(subject.points_mask.data.squeeze(0)).numpy()\n",
    "\n",
    "def transformPoints(pts, padding_params, cropping_params):\n",
    "    # Handle none types. Could have been handled in getCroppingParams but it's kept there for consistency with torchio\n",
    "    if padding_params is None:\n",
    "        padding_params = np.zeros(6)\n",
    "    if cropping_params is None:\n",
    "        cropping_params = np.zeros(6)\n",
    "\n",
    "    axis_add, axis_sub = padding_params[::2], cropping_params[::2]\n",
    "\n",
    "    pts = pts + axis_add - axis_sub # same as pts_trans[:,i] = pts_trans[:,i] + axis_add[i] - axis_sub[i] iterating over i\n",
    "    pts = pts[:,::-1]\n",
    "    return(pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.tensor([1,2,3])) == torch.Tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universalModels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
