{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio as tio\n",
    "from torchio.data.io import sitk_to_nib\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import SimpleITK as sitk\n",
    "import torch.nn.functional as F\n",
    "from itertools import product\n",
    "import edt\n",
    "from argparse import Namespace\n",
    "import pickle\n",
    "from torch import tensor\n",
    "from copy import deepcopy\n",
    "\n",
    "from utils.analysisUtils import compute_dice\n",
    "from utils.base_classes import Points, SegmenterWrapper, Inferer\n",
    "from utils.modelUtils import load_sammed3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_clicks3D(gt, n_clicks, seed = None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    volume_fg = np.where(gt==1) # Get foreground indices (formatted as triple of arrays)\n",
    "    volume_fg = np.array(volume_fg).T # Reformat to numpy array of shape n_fg_voxels x 3\n",
    "\n",
    "    n_fg_voxels = len(volume_fg)\n",
    "\n",
    "    # Error testing\n",
    "    if n_fg_voxels == 0:\n",
    "        raise RuntimeError(f'No foreground voxels found! Check that the supplied label is a binary segmentation mask with foreground coded as 1')\n",
    "\n",
    "    if n_fg_voxels < n_clicks:\n",
    "        raise RuntimeError(f'More foreground points were requested than the number of foreground voxels in the volume')\n",
    "\n",
    "    point_indices = np.random.choice(n_fg_voxels, size = n_clicks, replace = False)\n",
    "    pos_coords = volume_fg[point_indices]  \n",
    "    pos_coords = Points({'coords': pos_coords, 'labels': [1]*len(pos_coords)})\n",
    "    return(pos_coords)\n",
    "\n",
    "def get_next_click3D_torch_ritm(prev_seg, gt_semantic_seg):\n",
    "    mask_threshold = 0.5\n",
    "\n",
    "    batch_points = []\n",
    "    batch_labels = []\n",
    "    # dice_list = []\n",
    "\n",
    "    pred_masks = (prev_seg > mask_threshold)\n",
    "    true_masks = (gt_semantic_seg > 0)\n",
    "    fn_masks = torch.logical_and(true_masks, torch.logical_not(pred_masks))\n",
    "    fp_masks = torch.logical_and(torch.logical_not(true_masks), pred_masks)\n",
    "\n",
    "    fn_mask_single = F.pad(fn_masks, (1,1,1,1,1,1), 'constant', value=0).to(torch.uint8)[0,0]\n",
    "    fp_mask_single = F.pad(fp_masks, (1,1,1,1,1,1), 'constant', value=0).to(torch.uint8)[0,0]\n",
    "    fn_mask_dt = torch.tensor(edt.edt(fn_mask_single.cpu().numpy(), black_border=True, parallel=4))[1:-1, 1:-1, 1:-1]\n",
    "    fp_mask_dt = torch.tensor(edt.edt(fp_mask_single.cpu().numpy(), black_border=True, parallel=4))[1:-1, 1:-1, 1:-1]\n",
    "    fn_max_dist = torch.max(fn_mask_dt)\n",
    "    fp_max_dist = torch.max(fp_mask_dt)\n",
    "    is_positive = fn_max_dist > fp_max_dist # the biggest area is selected to be interaction point\n",
    "    dt = fn_mask_dt if is_positive else fp_mask_dt\n",
    "    to_point_mask = dt > (max(fn_max_dist, fp_max_dist) / 2.0) # use a erosion area\n",
    "    to_point_mask = to_point_mask[None, None]\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    for i in range(gt_semantic_seg.shape[0]):\n",
    "        points = torch.argwhere(to_point_mask[i])\n",
    "        point = points[np.random.randint(len(points))]\n",
    "        if fn_masks[i, 0, point[1], point[2], point[3]]:\n",
    "            is_positive = True\n",
    "        else:\n",
    "            is_positive = False\n",
    "\n",
    "        bp = point[1:].clone().detach().reshape(1,1,3) \n",
    "        bl = torch.tensor([int(is_positive),]).reshape(1,1)\n",
    "        batch_points.append(bp)\n",
    "        batch_labels.append(bl)\n",
    "\n",
    "    return batch_points, batch_labels # , (sum(dice_list)/len(dice_list)).item()    \n",
    "\n",
    "click_methods = {\n",
    "    'default': get_next_click3D_torch_ritm\n",
    "}\n",
    "\n",
    "def get_img_gt_sammed3d(img_path, gt_path):    \n",
    "    infer_transform = [\n",
    "        tio.ToCanonical(),\n",
    "    ]\n",
    "    transform = tio.Compose(infer_transform)\n",
    "\n",
    "    sitk_image = sitk.ReadImage(img_path)\n",
    "    sitk_label = sitk.ReadImage(gt_path)\n",
    "\n",
    "    if sitk_image.GetOrigin() != sitk_label.GetOrigin():\n",
    "        sitk_image.SetOrigin(sitk_label.GetOrigin())\n",
    "    if sitk_image.GetDirection() != sitk_label.GetDirection():\n",
    "        sitk_image.SetDirection(sitk_label.GetDirection())\n",
    "\n",
    "    sitk_image_arr, _ = sitk_to_nib(sitk_image)\n",
    "    sitk_label_arr, _ = sitk_to_nib(sitk_label)\n",
    "\n",
    "    subject = tio.Subject(\n",
    "        image = tio.ScalarImage(tensor=sitk_image_arr),\n",
    "        label = tio.LabelMap(tensor=sitk_label_arr),\n",
    "    )\n",
    "\n",
    "    if transform:\n",
    "        subject = transform(subject)\n",
    "                        \n",
    "    #subject = crop_transform(subject)\n",
    "\n",
    "    return subject.image.data.clone().detach(), subject.label.data.clone().detach()\n",
    "\n",
    "def preprocess_into_patches(img3D, pts_prompt = None, cheat = False, gt = None, offset_mode=\"center\"):\n",
    "    subject = tio.Subject(\n",
    "        image = tio.ScalarImage(tensor=img3D)\n",
    "    )\n",
    "    \n",
    "    if cheat:\n",
    "        subject.add_image(tio.LabelMap(tensor = gt,\n",
    "                                       affine = subject.image.affine,),\n",
    "                          image_name = 'label')\n",
    "        crop_transform = tio.CropOrPad(mask_name='label', \n",
    "                            target_shape=(128,128,128))\n",
    "    else:\n",
    "        coords = pts_prompt.value['coords']\n",
    "        crop_mask = torch.zeros_like(subject.image.data)\n",
    "        crop_mask[*coords.T] = 1\n",
    "        subject.add_image(tio.LabelMap(tensor = crop_mask,\n",
    "                                        affine = subject.image.affine),\n",
    "                            image_name=\"crop_mask\")\n",
    "        crop_transform = tio.CropOrPad(mask_name='crop_mask', \n",
    "                                target_shape=(128,128,128))\n",
    "        \n",
    "\n",
    "    padding_params, cropping_params = crop_transform.compute_crop_or_pad(subject)\n",
    "    # cropping_params: (x_start, x_max-(x_start+roi_size), y_start, ...)\n",
    "    # padding_params: (x_left_pad, x_right_pad, y_left_pad, ...)\n",
    "    if(cropping_params is None): cropping_params = (0,0,0,0,0,0)\n",
    "    if(padding_params is None): padding_params = (0,0,0,0,0,0)\n",
    "    roi_shape = crop_transform.target_shape\n",
    "    vol_bound = (0, img3D.shape[1], 0, img3D.shape[2], 0, img3D.shape[3])\n",
    "    center_oob_ori_roi=(\n",
    "        cropping_params[0]-padding_params[0], cropping_params[0]+roi_shape[0]-padding_params[0],\n",
    "        cropping_params[2]-padding_params[2], cropping_params[2]+roi_shape[1]-padding_params[2],\n",
    "        cropping_params[4]-padding_params[4], cropping_params[4]+roi_shape[2]-padding_params[4],\n",
    "    )\n",
    "    window_list = []\n",
    "    offset_dict = {\n",
    "        \"rounded\": list(product((-32,+32,0), repeat=3)),\n",
    "        \"center\": [(0,0,0)],\n",
    "    }\n",
    "    for offset in offset_dict[offset_mode]:\n",
    "        # get the position in original volume~(allow out-of-bound) for current offset\n",
    "        oob_ori_roi = (\n",
    "            center_oob_ori_roi[0]+offset[0], center_oob_ori_roi[1]+offset[0],\n",
    "            center_oob_ori_roi[2]+offset[1], center_oob_ori_roi[3]+offset[1],\n",
    "            center_oob_ori_roi[4]+offset[2], center_oob_ori_roi[5]+offset[2],\n",
    "        )\n",
    "        # get corresponing padding params based on `vol_bound`\n",
    "        padding_params = [0 for i in range(6)]\n",
    "        for idx, (ori_pos, bound) in enumerate(zip(oob_ori_roi, vol_bound)):\n",
    "            pad_val = 0\n",
    "            if(idx%2==0 and ori_pos<bound): # left bound\n",
    "                pad_val = bound-ori_pos\n",
    "            if(idx%2==1 and ori_pos>bound):\n",
    "                pad_val = ori_pos-bound\n",
    "            padding_params[idx] = pad_val\n",
    "        # get corresponding crop params after padding\n",
    "        cropping_params = (\n",
    "            oob_ori_roi[0]+padding_params[0], vol_bound[1]-oob_ori_roi[1]+padding_params[1],\n",
    "            oob_ori_roi[2]+padding_params[2], vol_bound[3]-oob_ori_roi[3]+padding_params[3],\n",
    "            oob_ori_roi[4]+padding_params[4], vol_bound[5]-oob_ori_roi[5]+padding_params[5],\n",
    "        )\n",
    "        # pad and crop for the original subject\n",
    "        pad_and_crop = tio.Compose([\n",
    "            tio.Pad(padding_params, padding_mode=crop_transform.padding_mode),\n",
    "            tio.Crop(cropping_params),\n",
    "        ])\n",
    "        subject_roi = pad_and_crop(subject)  \n",
    "        img3D_roi, = subject_roi.image.data.clone().detach().unsqueeze(0)\n",
    "        norm_transform = tio.ZNormalization(masking_method=lambda x: x > 0)\n",
    "        img3D_roi = norm_transform(img3D_roi) # (N, C, W, H, D)\n",
    "        img3D_roi = img3D_roi.unsqueeze(dim=0)\n",
    "        \n",
    "\n",
    "        # collect all position information, and set correct roi for sliding-windows in \n",
    "        # todo: get correct roi window of half because of the sliding \n",
    "        windows_clip = [0 for i in range(6)]\n",
    "        for i in range(3):\n",
    "            if(offset[i]<0):\n",
    "                windows_clip[2*i] = 0\n",
    "                windows_clip[2*i+1] = -(roi_shape[i]+offset[i])\n",
    "            elif(offset[i]>0):\n",
    "                windows_clip[2*i] = roi_shape[i]-offset[i]\n",
    "                windows_clip[2*i+1] = 0\n",
    "        pos3D_roi = dict(\n",
    "            padding_params=padding_params, cropping_params=cropping_params, \n",
    "            ori_roi=(\n",
    "                cropping_params[0]+windows_clip[0], cropping_params[0]+roi_shape[0]-padding_params[0]-padding_params[1]+windows_clip[1],\n",
    "                cropping_params[2]+windows_clip[2], cropping_params[2]+roi_shape[1]-padding_params[2]-padding_params[3]+windows_clip[3],\n",
    "                cropping_params[4]+windows_clip[4], cropping_params[4]+roi_shape[2]-padding_params[4]-padding_params[5]+windows_clip[5],\n",
    "            ),\n",
    "            pred_roi=(\n",
    "                padding_params[0]+windows_clip[0], roi_shape[0]-padding_params[1]+windows_clip[1],\n",
    "                padding_params[2]+windows_clip[2], roi_shape[1]-padding_params[3]+windows_clip[3],\n",
    "                padding_params[4]+windows_clip[4], roi_shape[2]-padding_params[5]+windows_clip[5],\n",
    "            ))\n",
    "\n",
    "        window_list.append((img3D_roi, pos3D_roi))\n",
    "    return cropping_params, padding_params, window_list\n",
    "\n",
    "def finetune_model_predict3D(img3D, sam_model_tune, prompt, device='cuda', prev_masks=None):\n",
    "    batch_points, batch_labels = prompt.value['coords'], prompt.value['labels']\n",
    "\n",
    "    if prev_masks is None:\n",
    "        prev_masks = torch.zeros_like(img3D)\n",
    "\n",
    "    low_res_masks = F.interpolate(prev_masks.float(), size=(128//4,128//4,128//4))\n",
    "\n",
    "    inputs = batch_points, batch_labels, low_res_masks, img3D\n",
    "    with open('/home/t722s/Desktop/test/inputs.pkl', 'wb') as f:\n",
    "        pickle.dump(inputs, f)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embedding = sam_model_tune.image_encoder(img3D.to(device)) # (1, 384, 16, 16, 16)\n",
    "    \n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        sparse_embeddings, dense_embeddings = sam_model_tune.prompt_encoder(\n",
    "            points=[batch_points, batch_labels],\n",
    "            boxes=None,\n",
    "            masks=low_res_masks.to(device),\n",
    "        )\n",
    "\n",
    "        low_res_masks, _ = sam_model_tune.mask_decoder(\n",
    "            image_embeddings=image_embedding.to(device), # (B, 384, 64, 64, 64)\n",
    "            image_pe=sam_model_tune.prompt_encoder.get_dense_pe(), # (1, 384, 64, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 384)\n",
    "            dense_prompt_embeddings=dense_embeddings, # (B, 384, 64, 64, 64)\n",
    "            multimask_output=False,\n",
    "            )\n",
    "        prev_masks = F.interpolate(low_res_masks, size=img3D.shape[-3:], mode='trilinear', align_corners=False)\n",
    "\n",
    "        medsam_seg_prob = torch.sigmoid(prev_masks)  # (B, 1, 64, 64, 64)\n",
    "        # convert prob to mask\n",
    "        medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "        medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "        \n",
    "    return medsam_seg\n",
    "\n",
    "def preprocess_prompt(pts_prompt, cropping_params, padding_params, use_only_first_point=False, device = 'cuda'):\n",
    "    coords = pts_prompt.value['coords']\n",
    "    labels = pts_prompt.value['labels']\n",
    "\n",
    "    point_offset = np.array([padding_params[0]-cropping_params[0], padding_params[2]-cropping_params[2], padding_params[4]-cropping_params[4]])\n",
    "    coords = coords[...,1:] + point_offset\n",
    "    \n",
    "    batch_points = torch.from_numpy(coords).unsqueeze(0).to(device)\n",
    "    batch_labels = torch.tensor(labels).unsqueeze(0).to(device)\n",
    "    if use_only_first_point: # use only the first point since the model wasn't trained to receive multiple points in one go \n",
    "        batch_points = batch_points[:, :1]\n",
    "        batch_labels = batch_labels[:, :1]\n",
    "    \n",
    "    pts_prompt = Points(value = {'coords': batch_points, 'labels': batch_labels})\n",
    "    return pts_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMMed3DWrapper(SegmenterWrapper):\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, img, prompt):\n",
    "        # Get prompt embeddings\n",
    "        ## Initialise empty prompts \n",
    "        coords, labs = None, None\n",
    "        boxes = None\n",
    "\n",
    "        ## Fill with relevant prompts\n",
    "        if isinstance(prompt, Points):\n",
    "            coords, labs = prompt.value['coords'], prompt.value['labels']\n",
    "\n",
    "        low_res_spatial_shape = [dim//4 for dim in img.shape[-3:]] #batch and channel dimensions remain the same, spatial dimensions are quartered \n",
    "        low_res_mask = torch.zeros([1,1] + low_res_spatial_shape).to(self.device) # [1,1] is batch and channel dimensions\n",
    "        inputs = coords, labs, low_res_mask, img\n",
    "        with open('/home/t722s/Desktop/test/inputs_new.pkl', 'wb') as f:\n",
    "            pickle.dump(inputs, f)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "                points = [coords, labs],\n",
    "                boxes = boxes,\n",
    "                masks = low_res_mask.to(self.device),\n",
    "            )\n",
    "\n",
    "            image_embedding = self.model.image_encoder(img) # (1, 384, 16, 16, 16)        \n",
    "        \n",
    "            ## Decode\n",
    "            mask_out, _ = self.model.mask_decoder(\n",
    "                image_embeddings = image_embedding.to(self.device), # (B, 384, 64, 64, 64)\n",
    "                image_pe = self.model.prompt_encoder.get_dense_pe(), # (1, 384, 64, 64, 64)\n",
    "                sparse_prompt_embeddings = sparse_embeddings, # (B, 2, 384)\n",
    "                dense_prompt_embeddings = dense_embeddings, # (B, 384, 64, 64, 64)\n",
    "                multimask_output = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "        logits = F.interpolate(mask_out, size=img.shape[-3:], mode = 'trilinear', align_corners = False).detach().cpu().squeeze()\n",
    "        \n",
    "        return(logits)\n",
    "    \n",
    "class SAMMed3DInferer(Inferer):\n",
    "    supported_prompts = supported_prompts = (Points,)\n",
    "    required_shape = (128, 128, 128) # Hard code to match training\n",
    "\n",
    "    def __init__(self, segmenter_wrapper: SAMMed3DWrapper, device = 'cuda', use_only_first_point = False):\n",
    "        self.segmenter = segmenter_wrapper\n",
    "        self.device = device\n",
    "        self.use_only_first_point = use_only_first_point\n",
    "        self.offset_mode = 'center'\n",
    "\n",
    "    def preprocess_into_patches(self, img3D, prompt = None, cheat = False, gt = None):\n",
    "        subject = tio.Subject(\n",
    "            image = tio.ScalarImage(tensor=img3D)\n",
    "        )\n",
    "        \n",
    "        if cheat:\n",
    "            subject.add_image(tio.LabelMap(tensor = gt,\n",
    "                                        affine = subject.image.affine,),\n",
    "                            image_name = 'label')\n",
    "            crop_transform = tio.CropOrPad(mask_name='label', \n",
    "                                target_shape=(128,128,128))\n",
    "        else:\n",
    "            coords = prompt.value['coords']\n",
    "            crop_mask = torch.zeros_like(subject.image.data)\n",
    "            crop_mask[*coords.T] = 1\n",
    "            subject.add_image(tio.LabelMap(tensor = crop_mask,\n",
    "                                            affine = subject.image.affine),\n",
    "                                image_name=\"crop_mask\")\n",
    "            crop_transform = tio.CropOrPad(mask_name='crop_mask', \n",
    "                                    target_shape=(128,128,128))\n",
    "            \n",
    "\n",
    "        padding_params, cropping_params = crop_transform.compute_crop_or_pad(subject)\n",
    "        # cropping_params: (x_start, x_max-(x_start+roi_size), y_start, ...)\n",
    "        # padding_params: (x_left_pad, x_right_pad, y_left_pad, ...)\n",
    "        if(cropping_params is None): cropping_params = (0,0,0,0,0,0)\n",
    "        if(padding_params is None): padding_params = (0,0,0,0,0,0)\n",
    "        roi_shape = crop_transform.target_shape\n",
    "        vol_bound = (0, img3D.shape[1], 0, img3D.shape[2], 0, img3D.shape[3])\n",
    "        center_oob_ori_roi=(\n",
    "            cropping_params[0]-padding_params[0], cropping_params[0]+roi_shape[0]-padding_params[0],\n",
    "            cropping_params[2]-padding_params[2], cropping_params[2]+roi_shape[1]-padding_params[2],\n",
    "            cropping_params[4]-padding_params[4], cropping_params[4]+roi_shape[2]-padding_params[4],\n",
    "        )\n",
    "        window_list = []\n",
    "        offset_dict = {\n",
    "            \"rounded\": list(product((-32,+32,0), repeat=3)),\n",
    "            \"center\": [(0,0,0)],\n",
    "        }\n",
    "        for offset in offset_dict[self.offset_mode]:\n",
    "            # get the position in original volume~(allow out-of-bound) for current offset\n",
    "            oob_ori_roi = (\n",
    "                center_oob_ori_roi[0]+offset[0], center_oob_ori_roi[1]+offset[0],\n",
    "                center_oob_ori_roi[2]+offset[1], center_oob_ori_roi[3]+offset[1],\n",
    "                center_oob_ori_roi[4]+offset[2], center_oob_ori_roi[5]+offset[2],\n",
    "            )\n",
    "            # get corresponing padding params based on `vol_bound`\n",
    "            padding_params = [0 for i in range(6)]\n",
    "            for idx, (ori_pos, bound) in enumerate(zip(oob_ori_roi, vol_bound)):\n",
    "                pad_val = 0\n",
    "                if(idx%2==0 and ori_pos<bound): # left bound\n",
    "                    pad_val = bound-ori_pos\n",
    "                if(idx%2==1 and ori_pos>bound):\n",
    "                    pad_val = ori_pos-bound\n",
    "                padding_params[idx] = pad_val\n",
    "            # get corresponding crop params after padding\n",
    "            cropping_params = (\n",
    "                oob_ori_roi[0]+padding_params[0], vol_bound[1]-oob_ori_roi[1]+padding_params[1],\n",
    "                oob_ori_roi[2]+padding_params[2], vol_bound[3]-oob_ori_roi[3]+padding_params[3],\n",
    "                oob_ori_roi[4]+padding_params[4], vol_bound[5]-oob_ori_roi[5]+padding_params[5],\n",
    "            )\n",
    "            # pad and crop for the original subject\n",
    "            pad_and_crop = tio.Compose([\n",
    "                tio.Pad(padding_params, padding_mode=crop_transform.padding_mode),\n",
    "                tio.Crop(cropping_params),\n",
    "            ])\n",
    "            subject_roi = pad_and_crop(subject)  \n",
    "            img3D_roi, = subject_roi.image.data.clone().detach().unsqueeze(0)\n",
    "            norm_transform = tio.ZNormalization(masking_method=lambda x: x > 0)\n",
    "            img3D_roi = norm_transform(img3D_roi) # (N, C, W, H, D)\n",
    "            img3D_roi = img3D_roi.unsqueeze(dim=0)\n",
    "            \n",
    "\n",
    "            # collect all position information, and set correct roi for sliding-windows in \n",
    "            # todo: get correct roi window of half because of the sliding \n",
    "            windows_clip = [0 for i in range(6)]\n",
    "            for i in range(3):\n",
    "                if(offset[i]<0):\n",
    "                    windows_clip[2*i] = 0\n",
    "                    windows_clip[2*i+1] = -(roi_shape[i]+offset[i])\n",
    "                elif(offset[i]>0):\n",
    "                    windows_clip[2*i] = roi_shape[i]-offset[i]\n",
    "                    windows_clip[2*i+1] = 0\n",
    "            pos3D_roi = dict(\n",
    "                padding_params=padding_params, cropping_params=cropping_params, \n",
    "                ori_roi=(\n",
    "                    cropping_params[0]+windows_clip[0], cropping_params[0]+roi_shape[0]-padding_params[0]-padding_params[1]+windows_clip[1],\n",
    "                    cropping_params[2]+windows_clip[2], cropping_params[2]+roi_shape[1]-padding_params[2]-padding_params[3]+windows_clip[3],\n",
    "                    cropping_params[4]+windows_clip[4], cropping_params[4]+roi_shape[2]-padding_params[4]-padding_params[5]+windows_clip[5],\n",
    "                ),\n",
    "                pred_roi=(\n",
    "                    padding_params[0]+windows_clip[0], roi_shape[0]-padding_params[1]+windows_clip[1],\n",
    "                    padding_params[2]+windows_clip[2], roi_shape[1]-padding_params[3]+windows_clip[3],\n",
    "                    padding_params[4]+windows_clip[4], roi_shape[2]-padding_params[5]+windows_clip[5],\n",
    "                ))\n",
    "\n",
    "            window_list.append((img3D_roi, pos3D_roi))\n",
    "        return cropping_params, padding_params, window_list\n",
    "\n",
    "    def preprocess_prompt(self, pts_prompt):\n",
    "        coords = pts_prompt.value['coords']\n",
    "        labels = pts_prompt.value['labels']\n",
    "\n",
    "        point_offset = np.array([self.padding_params[0]-self.cropping_params[0], self.padding_params[2]-self.cropping_params[2], self.padding_params[4]-self.cropping_params[4]])\n",
    "        coords = coords[...,1:] + point_offset\n",
    "        \n",
    "        batch_points = torch.from_numpy(coords).unsqueeze(0).to(self.device)\n",
    "        batch_labels = torch.tensor(labels).unsqueeze(0).to(self.device)\n",
    "        if self.use_only_first_point: # use only the first point since the model wasn't trained to receive multiple points in one go \n",
    "            batch_points = batch_points[:, :1]\n",
    "            batch_labels = batch_labels[:, :1]\n",
    "        \n",
    "        pts_prompt = Points(value = {'coords': batch_points, 'labels': batch_labels})\n",
    "        return pts_prompt\n",
    "    \n",
    "    def predict(self, img, prompt, cheat = False, gt = None):\n",
    "        if not isinstance(prompt, SAMMed3DInferer.supported_prompts):\n",
    "            raise ValueError(f'Unsupported prompt type: got {type(prompt)}')\n",
    "    \n",
    "        img, prompt = deepcopy(img), deepcopy(prompt)\n",
    "\n",
    "        self.cropping_params, self.padding_params, patch_list = self.preprocess_into_patches(img, prompt, cheat, gt)\n",
    "\n",
    "        prompt = self.preprocess_prompt(prompt)\n",
    "\n",
    "        pred  = torch.zeros_like(img).numpy()\n",
    "        for (image3D_patch, pos3D) in patch_list:\n",
    "            image3D_patch = image3D_patch.to(self.device)\n",
    "            logits = self.segmenter(image3D_patch, prompt)\n",
    "            seg_mask = (logits>0.5).numpy().astype(np.uint8)\n",
    "            ori_roi, pred_roi = pos3D[\"ori_roi\"], pos3D[\"pred_roi\"]\n",
    "            \n",
    "            seg_mask_roi = seg_mask[..., pred_roi[0]:pred_roi[1], pred_roi[2]:pred_roi[3], pred_roi[4]:pred_roi[5]]\n",
    "            pred[..., ori_roi[0]:ori_roi[1], ori_roi[2]:ori_roi[3], ori_roi[4]:ori_roi[5]] = seg_mask_roi\n",
    "        \n",
    "        return(pred.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/t722s/Desktop/UniversalModels/TrainedModels/sam_med3d_turbo.pth'\n",
    "device = 'cuda'\n",
    "sam_model_tune = load_sammed3d(checkpoint_path, device = device)\n",
    "wrapper = SAMMed3DWrapper(sam_model_tune, device)\n",
    "inferer = SAMMed3DInferer(wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9317160712240197"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = '/home/t722s/Desktop/Datasets/preprocessed/spleen/AbdomenAtlasJHU_2img/imagesTr/BDMAP_00000001.nii.gz'\n",
    "gt_path = '/home/t722s/Desktop/Datasets/preprocessed/spleen/AbdomenAtlasJHU_2img/labelsTr/BDMAP_00000001.nii.gz'\n",
    "img, gt = get_img_gt_sammed3d(img_path, gt_path)\n",
    "\n",
    "seed = 2024\n",
    "n = 5\n",
    "prompt = get_pos_clicks3D(gt, n, seed = seed)\n",
    "\n",
    "pred2 = inferer.predict(img, prompt, cheat = True, gt = gt)\n",
    "\n",
    "compute_dice(pred2, gt.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7685747088732163"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = '/home/t722s/Desktop/Datasets/preprocessed/aorta/AbdomenAtlasJHU_2img/imagesTr/BDMAP_00000002.nii.gz'\n",
    "gt_path = '/home/t722s/Desktop/Datasets/preprocessed/aorta/AbdomenAtlasJHU_2img/labelsTr/BDMAP_00000002.nii.gz'\n",
    "img, gt = get_img_gt_sammed3d(img_path, gt_path)\n",
    "\n",
    "seed = 2024\n",
    "n = 5\n",
    "prompt = get_pos_clicks3D(gt, n, seed = seed)\n",
    "\n",
    "offset_mode = 'center'\n",
    "cropping_params, padding_params, patch_list = preprocess_into_patches(img, pts_prompt = prompt, cheat = True, gt = gt, offset_mode=offset_mode)\n",
    "point_offset = np.array([padding_params[0]-cropping_params[0], padding_params[2]-cropping_params[2], padding_params[4]-cropping_params[4]])\n",
    "\n",
    "prompt = preprocess_prompt(prompt, cropping_params, padding_params)\n",
    "\n",
    "pred  = torch.zeros_like(gt).numpy()\n",
    "for (image3D_patch, pos3D) in patch_list:\n",
    "    seg_mask = finetune_model_predict3D(\n",
    "        image3D_patch, sam_model_tune, device=device, \n",
    "        prompt = prompt,\n",
    "        prev_masks=None)\n",
    "    \n",
    "    \n",
    "    ori_roi, pred_roi = pos3D[\"ori_roi\"], pos3D[\"pred_roi\"]\n",
    "    \n",
    "    seg_mask_roi = seg_mask[..., pred_roi[0]:pred_roi[1], pred_roi[2]:pred_roi[3], pred_roi[4]:pred_roi[5]]\n",
    "    pred[..., ori_roi[0]:ori_roi[1], ori_roi[2]:ori_roi[3], ori_roi[4]:ori_roi[5]] = seg_mask_roi\n",
    "\n",
    "compute_dice(pred, gt.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "points prompt({'coords': array([[  0, 116,  89,  45],\n",
       "       [  0, 123,  77,  78],\n",
       "       [  0, 121,  78,  74],\n",
       "       [  0, 114,  72,  84],\n",
       "       [  0, 115,  77,  63]]), 'labels': [1, 1, 1, 1, 1]})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/t722s/Desktop/Datasets/preprocessed/aorta/AbdomenAtlasJHU_2img/imagesTr/BDMAP_00000002.nii.gz'\n",
    "gt_path = '/home/t722s/Desktop/Datasets/preprocessed/aorta/AbdomenAtlasJHU_2img/labelsTr/BDMAP_00000002.nii.gz'\n",
    "img, gt = get_img_gt_sammed3d(img_path, gt_path)\n",
    "\n",
    "seed = 2024\n",
    "n = 5\n",
    "prompt = get_pos_clicks3D(gt, n, seed = seed)\n",
    "\n",
    "\n",
    "offset_mode = 'center'\n",
    "cropping_params, padding_params, patch_list = preprocess_into_patches(img, pts_prompt = prompt, cheat = True, gt = gt, offset_mode=offset_mode)\n",
    "point_offset = np.array([padding_params[0]-cropping_params[0], padding_params[2]-cropping_params[2], padding_params[4]-cropping_params[4]])\n",
    "\n",
    "prompt = preprocess_prompt(prompt, cropping_params, padding_params)\n",
    "\n",
    "pred  = torch.zeros_like(gt).numpy()\n",
    "for (image3D_patch, pos3D) in patch_list:\n",
    "    image3D_patch = image3D_patch.to(device)\n",
    "\n",
    "    logits = wrapper(image3D_patch, prompt)\n",
    "    seg_mask = (logits>0.5).numpy().astype(np.uint8)\n",
    "    ori_roi, pred_roi = pos3D[\"ori_roi\"], pos3D[\"pred_roi\"]\n",
    "    \n",
    "    seg_mask_roi = seg_mask[..., pred_roi[0]:pred_roi[1], pred_roi[2]:pred_roi[3], pred_roi[4]:pred_roi[5]]\n",
    "    pred[..., ori_roi[0]:ori_roi[1], ori_roi[2]:ori_roi[3], ori_roi[4]:ori_roi[5]] = seg_mask_roi\n",
    "\n",
    "compute_dice(pred, gt.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/home/t722s/Desktop/test/inputs_inferer.pkl', 'rb') as f:\n",
    "    inputs_inferer = pickle.load(f)\n",
    "with open('/home/t722s/Desktop/test/inputs_no_inferer.pkl', 'rb') as f:\n",
    "    inputs_no_inferer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 62,  68,  64],\n",
       "          [ 69,  56,  97],\n",
       "          [ 67,  57,  93],\n",
       "          [ 60,  51, 103],\n",
       "          [ 61,  56,  82]]], device='cuda:0'),\n",
       " tensor([[1, 1, 1, 1, 1]], device='cuda:0'),\n",
       " tensor([[[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]]], device='cuda:0'),\n",
       " tensor([[[[[ -1.0330,  -1.0330,  -1.0330,  ...,  -1.6976,  -1.7002,\n",
       "              -1.7631],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -1.9761,  -1.8716,\n",
       "              -1.6740],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -1.2983,  -1.3700,\n",
       "              -1.1935],\n",
       "            ...,\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ..., -10.0905, -10.2428,\n",
       "             -10.2489],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ..., -10.3450, -10.3090,\n",
       "             -10.1393],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ..., -10.0875, -10.0491,\n",
       "              -9.9217]],\n",
       " \n",
       "           [[ -1.0330,  -1.0330,  -1.0330,  ...,  -1.7741,  -1.8778,\n",
       "              -1.8316],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -1.4111,  -1.4657,\n",
       "              -1.2442],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.4684,  -0.5003,\n",
       "              -0.4572],\n",
       "            ...,\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -9.7907, -10.0803,\n",
       "             -10.2205],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ..., -10.3400, -10.3124,\n",
       "             -10.2370],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ..., -10.1894, -10.1109,\n",
       "              -9.9630]],\n",
       " \n",
       "           [[ -1.0330,  -1.0330,  -1.0330,  ...,  -1.2885,  -1.3729,\n",
       "              -1.4400],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.6879,  -0.6095,\n",
       "              -0.4838],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.3650,  -0.3921,\n",
       "              -0.4991],\n",
       "            ...,\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ..., -10.2005, -10.3085,\n",
       "             -10.2137],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ..., -10.2132, -10.3067,\n",
       "             -10.3187],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ..., -10.1341, -10.1991,\n",
       "             -10.0681]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[ -1.0330,  -1.0330,  -1.0330,  ...,  -0.5556,  -0.5395,\n",
       "              -0.7037],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.6035,  -0.6369,\n",
       "              -0.4534],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.5715,  -0.6745,\n",
       "              -0.5778],\n",
       "            ...,\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.1847,  -0.2167,\n",
       "              -0.2511],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.5172,  -0.5085,\n",
       "              -0.3669],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.4536,  -0.4433,\n",
       "              -0.4934]],\n",
       " \n",
       "           [[ -1.0330,  -1.0330,  -1.0330,  ...,  -0.5784,  -0.7063,\n",
       "              -0.7538],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.9595,  -0.8031,\n",
       "              -0.6291],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.6087,  -0.7419,\n",
       "              -0.7117],\n",
       "            ...,\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.0409,  -0.2328,\n",
       "              -0.3801],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.3598,  -0.4378,\n",
       "              -0.4230],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.5135,  -0.4824,\n",
       "              -0.6548]],\n",
       " \n",
       "           [[ -1.0330,  -1.0330,  -1.0330,  ...,  -0.5626,  -0.5622,\n",
       "              -0.6307],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.9357,  -0.8116,\n",
       "              -0.6087],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.9184,  -0.7696,\n",
       "              -0.7760],\n",
       "            ...,\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.3001,  -0.3726,\n",
       "              -0.4416],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.0962,  -0.2343,\n",
       "              -0.2961],\n",
       "            [ -1.0330,  -1.0330,  -1.0330,  ...,  -0.3288,  -0.4957,\n",
       "              -0.5545]]]]], device='cuda:0'))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_no_inferer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[61, 72, 44],\n",
       "          [68, 60, 77],\n",
       "          [66, 61, 73],\n",
       "          [59, 55, 83],\n",
       "          [60, 60, 62]]], device='cuda:0'),\n",
       " tensor([[1, 1, 1, 1, 1]], device='cuda:0'),\n",
       " tensor([[[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]]], device='cuda:0'),\n",
       " tensor([[[[[ -1.9249,  -1.9835,  -1.8682,  ...,  -1.5638,  -1.7584,\n",
       "              -1.8906],\n",
       "            [ -1.7639,  -1.6292,  -1.5993,  ...,  -2.0395,  -2.0752,\n",
       "              -1.9764],\n",
       "            [ -2.3172,  -2.2664,  -2.1530,  ...,  -1.8645,  -1.8810,\n",
       "              -2.0508],\n",
       "            ...,\n",
       "            [ -1.9940,  -2.0366,  -1.9732,  ...,  -8.8821,  -7.4648,\n",
       "              -7.0574],\n",
       "            [ -1.9914,  -1.9507,  -2.0394,  ...,  -7.6461,  -3.8969,\n",
       "              -1.6681],\n",
       "            [ -2.0287,  -1.8824,  -1.9246,  ...,  -9.8087,  -8.7704,\n",
       "              -7.5075]],\n",
       " \n",
       "           [[ -1.7252,  -1.7924,  -1.8026,  ...,  -1.6016,  -1.7585,\n",
       "              -2.0005],\n",
       "            [ -1.6927,  -1.8323,  -1.7209,  ...,  -2.0365,  -1.9838,\n",
       "              -1.8295],\n",
       "            [ -2.1652,  -2.1243,  -2.1928,  ...,  -1.6656,  -1.5730,\n",
       "              -1.7336],\n",
       "            ...,\n",
       "            [ -1.8084,  -1.8228,  -1.8239,  ..., -10.1329,  -9.4577,\n",
       "              -8.1443],\n",
       "            [ -1.8529,  -2.0162,  -1.9693,  ..., -10.1953,  -9.2737,\n",
       "              -6.3218],\n",
       "            [ -1.9109,  -2.0185,  -2.1929,  ..., -10.3857, -10.2398,\n",
       "              -9.5388]],\n",
       " \n",
       "           [[ -1.7853,  -1.6744,  -1.8364,  ...,  -1.8452,  -1.7774,\n",
       "              -1.9297],\n",
       "            [ -1.8876,  -1.9301,  -1.9422,  ...,  -1.9120,  -1.7835,\n",
       "              -1.6299],\n",
       "            [ -2.0305,  -2.0957,  -1.9754,  ...,  -1.6755,  -1.5107,\n",
       "              -1.6156],\n",
       "            ...,\n",
       "            [ -2.1831,  -2.0596,  -1.9366,  ..., -10.1502, -10.0628,\n",
       "              -9.5805],\n",
       "            [ -1.9447,  -2.0264,  -1.8389,  ...,  -9.9854, -10.2530,\n",
       "              -9.8953],\n",
       "            [ -2.0502,  -1.9507,  -1.9939,  ..., -10.2047, -10.0591,\n",
       "             -10.2440]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[ -1.8687,  -1.7609,  -1.8665,  ...,  -0.9034,  -1.0172,\n",
       "              -1.0150],\n",
       "            [ -1.9660,  -2.0523,  -1.7796,  ...,  -0.4475,  -0.3874,\n",
       "              -0.6259],\n",
       "            [ -2.0839,  -2.0565,  -2.1762,  ...,  -0.3661,  -0.4672,\n",
       "              -0.4977],\n",
       "            ...,\n",
       "            [ -1.8697,  -2.0056,  -2.1441,  ..., -10.1845, -10.1594,\n",
       "             -10.1282],\n",
       "            [ -1.9401,  -2.0456,  -1.9549,  ...,  -9.8775,  -9.9518,\n",
       "             -10.0789],\n",
       "            [ -2.0231,  -2.1379,  -2.0374,  ..., -10.1770, -10.1519,\n",
       "             -10.1223]],\n",
       " \n",
       "           [[ -1.8573,  -1.8261,  -1.9860,  ...,  -1.6156,  -1.6046,\n",
       "              -1.7118],\n",
       "            [ -1.6509,  -1.8195,  -1.8853,  ...,  -0.6870,  -0.6797,\n",
       "              -0.6387],\n",
       "            [ -1.9617,  -2.1453,  -2.1185,  ...,  -0.4063,  -0.5993,\n",
       "              -0.6856],\n",
       "            ...,\n",
       "            [ -2.1021,  -2.0601,  -2.0598,  ..., -10.2711, -10.1847,\n",
       "             -10.1078],\n",
       "            [ -1.9560,  -2.0122,  -2.0216,  ..., -10.1425, -10.1319,\n",
       "             -10.0022],\n",
       "            [ -2.1627,  -2.1419,  -2.1540,  ..., -10.2209, -10.2233,\n",
       "             -10.2000]],\n",
       " \n",
       "           [[ -2.0100,  -2.0973,  -2.1738,  ...,  -1.7729,  -1.6574,\n",
       "              -1.7017],\n",
       "            [ -1.7466,  -1.7836,  -1.7966,  ...,  -1.2968,  -1.3098,\n",
       "              -1.2413],\n",
       "            [ -1.8200,  -1.8235,  -1.8373,  ...,  -0.5078,  -0.4786,\n",
       "              -0.5919],\n",
       "            ...,\n",
       "            [ -1.9787,  -1.9190,  -1.9620,  ..., -10.3217, -10.2097,\n",
       "             -10.1269],\n",
       "            [ -1.9574,  -1.9705,  -2.0970,  ..., -10.2773, -10.2835,\n",
       "             -10.2745],\n",
       "            [ -2.0791,  -2.0777,  -1.8772,  ..., -10.2942, -10.2084,\n",
       "             -10.2217]]]]], device='cuda:0'))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_inferer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(sliding_window = False,\n",
    "                 num_clicks = 1,\n",
    "                 point_method = 'default',\n",
    "                 crop_size = 128,\n",
    "                 seed = seed,\n",
    "                 test_data_path = '/home/t722s/Desktop/Datasets/preprocessed/')\n",
    "def run_inference(img_path, gt_path, sam_model_tune, n = 5, seed = 2024):\n",
    "    img, gt = get_img_gt_sammed3d(img_path, gt_path)\n",
    "\n",
    "    pts_prompt = get_pos_clicks3D(gt, n, seed = seed)\n",
    "\n",
    "    offset_mode = 'center'\n",
    "    cropping_params, padding_params, patch_list = preprocess_into_patches(img, pts_prompt = pts_prompt, cheat = True, gt = gt, offset_mode=offset_mode)\n",
    "\n",
    "    pts_prompt = preprocess_prompt(pts_prompt, cropping_params, padding_params)\n",
    "\n",
    "    pred  = torch.zeros_like(gt).numpy()\n",
    "    for (image3D_patch, pos3D) in patch_list:\n",
    "        image3D_patch = image3D_patch.to(device)\n",
    "        logits = wrapper(image3D_patch, pts_prompt)\n",
    "        seg = (logits > 0.5).numpy().astype(np.uint8)\n",
    "\n",
    "        ori_roi, pred_roi = pos3D[\"ori_roi\"], pos3D[\"pred_roi\"]\n",
    "        \n",
    "        seg_mask_roi = seg[..., pred_roi[0]:pred_roi[1], pred_roi[2]:pred_roi[3], pred_roi[4]:pred_roi[5]]\n",
    "        pred[..., ori_roi[0]:ori_roi[1], ori_roi[2]:ori_roi[3], ori_roi[4]:ori_roi[5]] = seg_mask_roi\n",
    "\n",
    "    res = compute_dice(pred, gt.numpy())\n",
    "    return(pred, res)\n",
    "from glob import glob\n",
    "import os.path as osp\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "join = osp.join\n",
    "args = Namespace(sliding_window = False,\n",
    "                    num_clicks = 1,\n",
    "                    point_method = 'default',\n",
    "                    crop_size = 128,\n",
    "                    seed = seed,\n",
    "                    test_data_path = '/home/t722s/Desktop/Datasets/preprocessed/')\n",
    "\n",
    "all_dataset_paths = glob(join(args.test_data_path, \"*\", \"*\"))\n",
    "all_dataset_paths = list(filter(osp.isdir, all_dataset_paths))\n",
    "all_dataset_paths =[p for p in all_dataset_paths if not 'background' in p]\n",
    "\n",
    "organs = [f.split('/')[-2] for f in all_dataset_paths]\n",
    "\n",
    "res_dict = {organ: {} for organ in organs}\n",
    "for ds in all_dataset_paths:\n",
    "    imgs_dir = join(ds,'imagesTr')\n",
    "    imgs = [join(imgs_dir, img) for img in sorted(os.listdir(imgs_dir))]\n",
    "\n",
    "    for img_path in imgs:\n",
    "        gt_path = img_path.replace('imagesTr', 'labelsTr')\n",
    "        print(f'Inferring on {img_path}')\n",
    "        _, dice = run_inference(img_path, gt_path, sam_model_tune, seed = 2024)\n",
    "\n",
    "        organ = img_path.split('/')[-4]\n",
    "        res_dict[organ][os.path.basename(img_path)] = dice\n",
    "\n",
    "res_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universalModels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
