{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import json\n",
    "import torchio as tio\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from typing import TypeVar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/t722s/Desktop/Datasets/BratsMini/imagesTs/BraTS2021_01646.nii.gz'\n",
    "label_path = '/home/t722s/Desktop/Datasets/BratsMini/labelsTs/BraTS2021_01646.nii.gz'\n",
    "prompts_path = '/home/t722s/Desktop/Sam-Med3DTest/BratsMini/prompts.pkl'\n",
    "metadata_path = '/home/t722s/Desktop/Datasets/BratsMini/dataset.json'\n",
    "\n",
    "with open(prompts_path, 'rb') as f:\n",
    "    prompts_dict = pickle.load(f)\n",
    "\n",
    "with open(metadata_path, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "img_name = os.path.basename(img_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt(): # Abstract class for prompts to be inputted with the SAM adjusted model\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.name} prompt({self.value}) '\n",
    "\n",
    "    def _get_crop_pad_center(self): \n",
    "        \"\"\"Calculate and return an approximate center of the region of interest (ROI).\"\"\"\n",
    "        raise NotImplementedError('This class does not have an implemented method to get the crop/pad center')\n",
    "\n",
    "    def _transform_xyz_to_zyx(self): \n",
    "        \"\"\"Transform coordinates from xyz to zyx.\"\"\"\n",
    "        raise NotImplementedError('This class does not have an implemented method to change xyz to zyx')\n",
    "\n",
    "    def _transform_with_crop_pad(self, cropping_params, padding_params):\n",
    "        \"\"\"Adjust the prompt according to cropping and padding parameters.\"\"\"\n",
    "        raise NotImplementedError('This class does not have an implemented method to appropriately change the prompt due to a cropping/padding')\n",
    "\n",
    "class Points(Prompt):\n",
    "    def __init__(self, value):\n",
    "        super().__init__(name = 'points', value = value)\n",
    "\n",
    "    def _transform_xyz_to_zyx(self): # In place method\n",
    "        self.value['points'] = self.value['points'][:,::-1]\n",
    "\n",
    "    def _get_crop_pad_center(self):\n",
    "        bbox_min = self.value['points'].T.min(axis = 1) # Get an array of two points: the minimal and maximal vertices of the minimal cube parallel to the axes bounding the points\n",
    "        bbox_max = self.value['points'].T.max(axis = 1) + 1 # Add 1 since we'll be using this for indexing # TESTING: Remove 'self's here\n",
    "        point_center = np.mean((bbox_min, bbox_max), axis = 0)  \n",
    "\n",
    "        return(point_center)\n",
    "\n",
    "    def _transform_with_crop_pad(self, cropping_params, padding_params):\n",
    "        axis_add, axis_sub = padding_params[::2], cropping_params[::2] \n",
    "        self.value['points'] = self.value['points'] + axis_add - axis_sub # same as value[:,i] = value[:,i] + axis_add[i] - axis_sub[i] iterating over i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(gt, organ, metadata): # TODO: Raise errors for metadata not having labels, labels in the wrong k,v order, and organ not being present\n",
    "    organ_label = int(metadata['labels'][organ])\n",
    "    gt_binary = torch.where(gt == organ_label, 1, torch.zeros_like(gt))\n",
    "    return(gt_binary)\n",
    "\n",
    "class SegmenterData():\n",
    "    def __init__(self, img_path, prompt, cropped_shape = None, crop_pad_center = None, xyz = True, standardize = True,\n",
    "                 prev_mask = None, img_embedding = None, ground_truth = None): # xyz: if prompts and images are in xyz: need to be converted to zyx\n",
    "        # Store supplied variables\n",
    "        self.img_path = img_path\n",
    "        self.img_name = os.path.basename(self.img_path)\n",
    "        self.img = sitk.GetArrayFromImage(sitk.ReadImage(img_path))\n",
    "        self.prompt = prompt\n",
    "        \n",
    "        self.crop_pad_center = crop_pad_center\n",
    "        self.cropped_shape = (cropped_shape, cropped_shape, cropped_shape) if isinstance(cropped_shape, int) else cropped_shape\n",
    "        \n",
    "        self.prev_mask = prev_mask\n",
    "        self.img_embedding = img_embedding\n",
    "\n",
    "        # Store originals without any procerssing\n",
    "        self.prompt_original, self.img_original = copy.deepcopy(prompt), copy.deepcopy(self.img)\n",
    "\n",
    "        # Points generated from generate_points.py are in xyz since the image is read in through sitk.GetArrayFromImage on sitk.ReadImage output, and here self.img is also in xyz. Transform both to zyx\n",
    "        if xyz:\n",
    "            self.img = self.img.transpose(2,1,0)\n",
    "            prompt._transform_xyz_to_zyx()\n",
    "\n",
    "        if crop_pad_center == 'from_prompt': \n",
    "            self.crop_pad_center = prompt._get_crop_pad_center()\n",
    "\n",
    "        if self.crop_pad_center is not None:\n",
    "            self.crop_params, self.pad_params = self._get_crop_pad_params(self.crop_pad_center, self.cropped_shape)\n",
    "            # Crop/pad image\n",
    "            self.img = self._crop_im(self.img, self.crop_params)\n",
    "            self.img = self._pad_im(self.img, self.pad_params)\n",
    "\n",
    "            # Crop/pad prompts\n",
    "            self.prompt._transform_with_crop_pad(self.crop_params, self.pad_params)\n",
    "\n",
    "        if standardize == True:\n",
    "            mask = self.img > 0\n",
    "            mean, std = self.img[mask].mean(), self.img[mask].std()\n",
    "            # standardize_func = tio.ZNormalization(masking_method=lambda x: x > 0)\n",
    "            # self.img2 = np.array(standardize_func(torch.from_numpy(self.img).unsqueeze(0))).squeeze(0) # Gives a different result; investigate\n",
    "            self.img = (self.img-mean)/std \n",
    "\n",
    "    def _get_crop_pad_params(self, crop_pad_center, target_shape): # Modified from TorchIO cropOrPad\n",
    "        subject_shape = self.img.shape\n",
    "        padding = []\n",
    "        cropping = []\n",
    "\n",
    "        for dim in range(3):\n",
    "            target_dim = target_shape[dim]\n",
    "            center_dim = crop_pad_center[dim]\n",
    "            subject_dim = subject_shape[dim]\n",
    "\n",
    "            center_on_index = not (center_dim % 1)\n",
    "            target_even = not (target_dim % 2)\n",
    "\n",
    "            # Approximation when the center cannot be computed exactly\n",
    "            # The output will be off by half a voxel, but this is just an\n",
    "            # implementation detail\n",
    "            if target_even ^ center_on_index:\n",
    "                center_dim -= 0.5\n",
    "\n",
    "            begin = center_dim - target_dim / 2\n",
    "            if begin >= 0:\n",
    "                crop_ini = begin\n",
    "                pad_ini = 0\n",
    "            else:\n",
    "                crop_ini = 0\n",
    "                pad_ini = -begin\n",
    "\n",
    "            end = center_dim + target_dim / 2\n",
    "            if end <= subject_dim:\n",
    "                crop_fin = subject_dim - end\n",
    "                pad_fin = 0\n",
    "            else:\n",
    "                crop_fin = 0\n",
    "                pad_fin = end - subject_dim\n",
    "\n",
    "            padding.extend([pad_ini, pad_fin])\n",
    "            cropping.extend([crop_ini, crop_fin])\n",
    "\n",
    "        padding_params = np.asarray(padding, dtype=int)\n",
    "        cropping_params = np.asarray(cropping, dtype=int)\n",
    "\n",
    "        return cropping_params, padding_params  # type: ignore[return-value]\n",
    "\n",
    "    def _crop_im(self, img, cropping_params): # Modified from TorchIO cropOrPad\n",
    "        low = cropping_params[::2]\n",
    "        high = cropping_params[1::2]\n",
    "        index_ini = low\n",
    "        index_fin = np.array(img.shape) - high \n",
    "        i0, j0, k0 = index_ini\n",
    "        i1, j1, k1 = index_fin\n",
    "        image_cropped = img[i0:i1, j0:j1, k0:k1]\n",
    "\n",
    "        return(image_cropped)\n",
    "\n",
    "    def _pad_im(self, img, padding_params): # Modified from TorchIO cropOrPad\n",
    "        paddings = padding_params[:2], padding_params[2:4], padding_params[4:]\n",
    "        image_padded = np.pad(img, paddings, mode = 'constant', constant_values = 0)  \n",
    "\n",
    "        return(image_padded)\n",
    "    \n",
    "    def invert_crop_or_pad(self, mask, cropping_params, padding_params):\n",
    "        if padding_params is not None:\n",
    "            mask = self._crop_im(mask, padding_params)\n",
    "        if cropping_params is not None:\n",
    "            mask = self._pad_im(mask, cropping_params)\n",
    "        return(mask)\n",
    "\n",
    "    def clear_storage(self):\n",
    "        self.prev_mask = None\n",
    "        self.image_embedding = None\n",
    "\n",
    "    def generate_next_prompt(self):\n",
    "        '''Generate next prompt as a function of prev mask and ground truth'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        '''Any further necessary preprocessing steps'''\n",
    "        pass\n",
    "\n",
    "    def postprocess_mask(self, mask):\n",
    "        '''Any further necessary postprocessing steps'''\n",
    "        pass\n",
    " \n",
    "    def predict(self, model, inputs, device = 'cuda', keep_img_embedding = True):\n",
    "        '''Obtain logits '''\n",
    "        pass\n",
    "\n",
    "SAM3D = TypeVar('SAM3D')\n",
    "\n",
    "class SAMMed3DSegmenter(Backbone):\n",
    "    def __init__(self, model: SAM3D, required_shape, device = 'cuda'):\n",
    "        self.model = model\n",
    "        self.prev_mask = None\n",
    "        self.required_shape = required_shape\n",
    "        self.inputs = None\n",
    "        self.device = 'cuda'\n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        '''Any further necessary preprocessing steps'''\n",
    "        img = torch.from_numpy(img).unsqueeze(0).unsqueeze(0) # add channel and batch dimensions\n",
    "        return(img)\n",
    "    \n",
    "    def preprocess_points(self, prompt): # We only make use of point prompts\n",
    "        pts = torch.from_numpy(prompt.value['points']).unsqueeze(0).to(self.device) # Must have shape B, N, 3\n",
    "        labs = torch.tensor(prompt.value['labels']).unsqueeze(0).to(self.device)\n",
    "        return(pts, labs)\n",
    "    \n",
    "    def postprocess_logits(self, mask, cropping_params, padding_params, xyz):\n",
    "        mask = torch.sigmoid(mask)\n",
    "        mask = (mask>0.5).numpy().astype(np.uint8)\n",
    "        mask = self.inputs.invert_crop_or_pad(mask, cropping_params, padding_params)\n",
    "        if xyz:\n",
    "            mask = mask.transpose(2,1,0)\n",
    "        return(mask)\n",
    " \n",
    "    def predict(self, inputs, store_inference_data, device = 'cuda',):\n",
    "        # Last preprocessing steps for prompts and image\n",
    "        self.inputs = inputs\n",
    "        img = self.preprocess_img(inputs.img)\n",
    "        pts, labs = self.preprocess_points(inputs.prompt)\n",
    "\n",
    "        # Obtain necessaries for inference and then get output mask\n",
    "        ## Handle mask and embeddings\n",
    "        low_res_spatial_shape = [dim//4 for dim in img.shape[-3:]] #batch and channel dimensions remain the same, spatial dimensions are quartered \n",
    "        \n",
    "        if inputs.prev_mask is None:\n",
    "            low_res_mask = torch.zeros([1,1] + low_res_spatial_shape).to(self.device) # Include batch and channel dimensions\n",
    "        else:\n",
    "            print('Using previous mask as an input!')\n",
    "            low_res_mask = F.interpolate(inputs.prev_mask, size = low_res_spatial_shape, mode='trilinear', align_corners=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_embedding = self.model.image_encoder(img.to(self.device)) # (1, 384, 16, 16, 16)\n",
    "\n",
    "        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "                points = [pts, labs],\n",
    "                boxes = None,\n",
    "                masks = low_res_mask.to(self.device),\n",
    "            )\n",
    "        \n",
    "        ## Decode\n",
    "        mask_out, _ = self.model.mask_decoder(\n",
    "            image_embeddings = image_embedding.to(self.device), # (B, 384, 64, 64, 64)\n",
    "            image_pe = self.model.prompt_encoder.get_dense_pe(), # (1, 384, 64, 64, 64)\n",
    "            sparse_prompt_embeddings = sparse_embeddings, # (B, 2, 384)\n",
    "            dense_prompt_embeddings = dense_embeddings, # (B, 384, 64, 64, 64)\n",
    "            multimask_output = False,\n",
    "            )\n",
    "        \n",
    "        # If repeating inference on same image, store the mask and the embedding\n",
    "        \n",
    "        if store_inference_data == True:\n",
    "            inputs.prev_mask = mask_out\n",
    "            inputs.image_embedding = image_embedding\n",
    "        \n",
    "        logits = F.interpolate(mask_out, size=img.shape[-3:], mode = 'trilinear', align_corners = False).detach().cpu().squeeze()\n",
    "        segmentation = self.postprocess_logits(logits, inputs.crop_params, inputs.pad_params, xyz = True)\n",
    "\n",
    "        return(segmentation)\n",
    "    \n",
    "    def clear_storage(self):\n",
    "        self.prev_mask = None\n",
    "        self.image_embedding = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.build_sam3D import build_sam3D_vit_b_ori\n",
    "\n",
    "device = 'cuda'\n",
    "checkpoint_path = '/home/t722s/Desktop/UniversalModels/TrainedModels/sam_med3d.pth'\n",
    "model_type = 'vit_b_ori'\n",
    "# Load in model\n",
    "\n",
    "sam_model_tune = build_sam3D_vit_b_ori(checkpoint=None).to(device)\n",
    "if checkpoint_path is not None:\n",
    "    model_dict = torch.load(checkpoint_path, map_location=device)\n",
    "    state_dict = model_dict['model_state_dict']\n",
    "    sam_model_tune.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dice(mask_gt, mask_pred):\n",
    "    \"\"\"Compute soerensen-dice coefficient.\n",
    "    Returns:\n",
    "    the dice coeffcient as float. If both masks are empty, the result is NaN\n",
    "    \"\"\"\n",
    "    volume_sum = mask_gt.sum() + mask_pred.sum()\n",
    "    if volume_sum == 0:\n",
    "        return np.NaN\n",
    "    volume_intersect = (mask_gt & mask_pred).sum()\n",
    "    return 2*volume_intersect / volume_sum\n",
    "\n",
    "gt = sitk.GetArrayFromImage(sitk.ReadImage(label_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 31  0 34  2  0] [ 0  0  2  0  0 26] (160, 160, 104)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7024892502998216"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pts = prompts_dict[img_name][1]['3D']\n",
    "pts_prompt = Points({'points': pts[0], 'labels': pts[1]})\n",
    "inputs = SegmenterData(img_path, pts_prompt, 128, 'from_prompt')\n",
    "segmenter = SAMMed3DSegmenter(sam_model_tune, (128,128,128))\n",
    "segmentation = segmenter.predict(inputs, store_inference_data=True)\n",
    "compute_dice(segmentation, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using previous mask as an input!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6565609307258039"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentation2 = segmenter.predict(inputs, store_inference_data=True)\n",
    "compute_dice(segmentation2, gt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universalModels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
