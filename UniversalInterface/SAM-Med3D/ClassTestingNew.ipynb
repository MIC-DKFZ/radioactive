{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import json\n",
    "import torchio as tio\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from typing import TypeVar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(gt, organ, metadata): # TODO: Raise errors for metadata not having labels, labels in the wrong k,v order, and organ not being present\n",
    "    organ_label = int(metadata['labels'][organ])\n",
    "    gt_binary = torch.where(gt == organ_label, 1, torch.zeros_like(gt))\n",
    "    return(gt_binary)\n",
    "\n",
    "def get_crop_pad_center_from_points(points):\n",
    "    bbox_min = points.value['coords'].T.min(axis = 1) # Get an array of two points: the minimal and maximal vertices of the minimal cube parallel to the axes bounding the points\n",
    "    bbox_max = points.value['coords'].T.max(axis = 1) + 1 # Add 1 since we'll be using this for indexing # TESTING: Remove 'self's here\n",
    "    point_center = np.mean((bbox_min, bbox_max), axis = 0)  \n",
    "\n",
    "    return(point_center)\n",
    "\n",
    "def get_crop_pad_params(img, crop_pad_center, target_shape): # Modified from TorchIO cropOrPad\n",
    "    subject_shape = img.shape\n",
    "    padding = []\n",
    "    cropping = []\n",
    "\n",
    "    for dim in range(3):\n",
    "        target_dim = target_shape[dim]\n",
    "        center_dim = crop_pad_center[dim]\n",
    "        subject_dim = subject_shape[dim]\n",
    "\n",
    "        center_on_index = not (center_dim % 1)\n",
    "        target_even = not (target_dim % 2)\n",
    "\n",
    "        # Approximation when the center cannot be computed exactly\n",
    "        # The output will be off by half a voxel, but this is just an\n",
    "        # implementation detail\n",
    "        if target_even ^ center_on_index:\n",
    "            center_dim -= 0.5\n",
    "\n",
    "        begin = center_dim - target_dim / 2\n",
    "        if begin >= 0:\n",
    "            crop_ini = begin\n",
    "            pad_ini = 0\n",
    "        else:\n",
    "            crop_ini = 0\n",
    "            pad_ini = -begin\n",
    "\n",
    "        end = center_dim + target_dim / 2\n",
    "        if end <= subject_dim:\n",
    "            crop_fin = subject_dim - end\n",
    "            pad_fin = 0\n",
    "        else:\n",
    "            crop_fin = 0\n",
    "            pad_fin = end - subject_dim\n",
    "\n",
    "        padding.extend([pad_ini, pad_fin])\n",
    "        cropping.extend([crop_ini, crop_fin])\n",
    "\n",
    "    padding_params = np.asarray(padding, dtype=int)\n",
    "    cropping_params = np.asarray(cropping, dtype=int)\n",
    "\n",
    "    return cropping_params, padding_params  # type: ignore[return-value]\n",
    "\n",
    "def crop_im(img, cropping_params): # Modified from TorchIO cropOrPad\n",
    "        low = cropping_params[::2]\n",
    "        high = cropping_params[1::2]\n",
    "        index_ini = low\n",
    "        index_fin = np.array(img.shape) - high \n",
    "        i0, j0, k0 = index_ini\n",
    "        i1, j1, k1 = index_fin\n",
    "        image_cropped = img[i0:i1, j0:j1, k0:k1]\n",
    "\n",
    "        return(image_cropped)\n",
    "\n",
    "def pad_im(img, padding_params): # Modified from TorchIO cropOrPad\n",
    "    paddings = padding_params[:2], padding_params[2:4], padding_params[4:]\n",
    "    image_padded = np.pad(img, paddings, mode = 'constant', constant_values = 0)  \n",
    "\n",
    "    return(image_padded)\n",
    "\n",
    "def crop_pad_coords(coords, cropping_params, padding_params):\n",
    "    axis_add, axis_sub = padding_params[::2], cropping_params[::2] \n",
    "    coords = coords + axis_add - axis_sub # same as value[:,i] = value[:,i] + axis_add[i] - axis_sub[i] iterating over i\n",
    "    return(coords)\n",
    "\n",
    "def invert_crop_or_pad(mask, cropping_params, padding_params):\n",
    "    if padding_params is not None:\n",
    "        mask = crop_im(mask, padding_params)\n",
    "    if cropping_params is not None:\n",
    "        mask = pad_im(mask, cropping_params)\n",
    "    return(mask)\n",
    "\n",
    "def compute_dice(mask_gt, mask_pred):\n",
    "    \"\"\"Compute soerensen-dice coefficient.\n",
    "    Returns:\n",
    "    the dice coeffcient as float. If both masks are empty, the result is NaN\n",
    "    \"\"\"\n",
    "    volume_sum = mask_gt.sum() + mask_pred.sum()\n",
    "    if volume_sum == 0:\n",
    "        return np.NaN\n",
    "    volume_intersect = (mask_gt & mask_pred).sum()\n",
    "    return 2*volume_intersect / volume_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt(): # Abstract class for prompts to be inputted with the SAM adjusted model\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.name} prompt({self.value})'\n",
    "\n",
    "class Points(Prompt):\n",
    "    def __init__(self, value):\n",
    "        super().__init__(name = 'points', value = value)\n",
    "        \n",
    "class Backbone():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        '''Any necessary preprocessing steps'''\n",
    "        pass\n",
    "\n",
    "    def postprocess_mask(self, mask):\n",
    "        '''Any necessary postprocessing steps'''\n",
    "        pass\n",
    " \n",
    "    def predict(self, model, inputs, device = 'cuda', keep_img_embedding = True):\n",
    "        '''Obtain logits '''\n",
    "        pass\n",
    "\n",
    "SAM3D = TypeVar('SAM3D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything.build_sam3D import build_sam3D_vit_b_ori\n",
    "\n",
    "device = 'cuda'\n",
    "checkpoint_path = '/home/t722s/Desktop/UniversalModels/TrainedModels/sam_med3d.pth'\n",
    "model_type = 'vit_b_ori'\n",
    "# Load in model\n",
    "\n",
    "sam_model_tune = build_sam3D_vit_b_ori(checkpoint=None).to(device)\n",
    "if checkpoint_path is not None:\n",
    "    model_dict = torch.load(checkpoint_path, map_location=device)\n",
    "    state_dict = model_dict['model_state_dict']\n",
    "    sam_model_tune.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMMed3DSegmenter(Backbone):\n",
    "    def __init__(self, model: SAM3D, required_shape, device = 'cuda'):\n",
    "        self.model = model\n",
    "        self.prev_mask = None\n",
    "        self.required_shape = required_shape\n",
    "        self.inputs = None\n",
    "        self.device = device\n",
    "\n",
    "    def preprocess_img(self, img, crop_params, pad_params):\n",
    "        '''Any necessary preprocessing steps'''\n",
    "\n",
    "        img = crop_im(img, crop_params) \n",
    "        img = pad_im(img, pad_params)\n",
    "\n",
    "        mask = img > 0\n",
    "        mean, std = img[mask].mean(), img[mask].std()\n",
    "        # standardize_func = tio.ZNormalization(masking_method=lambda x: x > 0)\n",
    "        # img2 = np.array(standardize_func(torch.from_numpy(img).unsqueeze(0))).squeeze(0) # Gives a different result; investigate\n",
    "        img = (img-mean)/std \n",
    "\n",
    "        img = torch.from_numpy(img).unsqueeze(0).unsqueeze(0).to(device) # add channel and batch dimensions\n",
    "        return(img)\n",
    "    \n",
    "    def preprocess_points(self, points, crop_params, pad_params): # We only make use of point prompts\n",
    "        coords, labs = points.value['coords'], points.value['labels']\n",
    "\n",
    "        coords = crop_pad_coords(coords, crop_params, pad_params)\n",
    "\n",
    "        coords = torch.from_numpy(coords).unsqueeze(0).to(self.device) # Must have shape B, N, 3\n",
    "        labs = torch.tensor(labs).unsqueeze(0).to(self.device)\n",
    "        return(coords, labs)\n",
    "    \n",
    "    def postprocess_logits(self, mask, cropping_params, padding_params):\n",
    "        mask = torch.sigmoid(mask)\n",
    "        mask = (mask>0.5).numpy().astype(np.uint8)\n",
    "        mask = invert_crop_or_pad(mask, cropping_params, padding_params)\n",
    "\n",
    "        mask = mask.transpose(2,1,0)\n",
    "        return(mask)\n",
    " \n",
    "    def predict(self, img, prompt, store_inference_data, use_prev_mask, use_prev_embedding, device = 'cuda',):\n",
    "        # Last preprocessing steps for prompts and image\n",
    "        if not isinstance(prompt, Points):\n",
    "            raise RuntimeError('Currently only points are supported')\n",
    "\n",
    "        self.crop_pad_center = get_crop_pad_center_from_points(prompt)\n",
    "        self.crop_params, self.pad_params = get_crop_pad_params(img, self.crop_pad_center, self.required_shape)\n",
    "        img = self.preprocess_img(img, self.crop_params, self.pad_params)\n",
    "        pts, labs = self.preprocess_points(prompt, self.crop_params, self.pad_params)\n",
    "\n",
    "        # Obtain necessaries for inference and then get output mask\n",
    "        ## Handle mask and embeddings\n",
    "        low_res_spatial_shape = [dim//4 for dim in img.shape[-3:]] #batch and channel dimensions remain the same, spatial dimensions are quartered \n",
    "        \n",
    "        if use_prev_mask:\n",
    "            if self.prev_mask is not None:\n",
    "                print('Using previous mask as an input!')\n",
    "                low_res_mask = F.interpolate(self.prev_mask, size = low_res_spatial_shape, mode='trilinear', align_corners=False)\n",
    "            else:\n",
    "                raise RuntimeError('Tried to use previous mask, but no mask is stored.')\n",
    "        else:\n",
    "            low_res_mask = torch.zeros([1,1] + low_res_spatial_shape).to(self.device) # [1,1] is batch and channel dimensions\n",
    "\n",
    "        if use_prev_embedding:\n",
    "            if self.image_embedding is not None:\n",
    "                image_embedding = self.image_embedding\n",
    "            else:\n",
    "                raise RuntimeError('Tried to use previous embedding, but no embedding is stored.')\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                image_embedding = self.model.image_encoder(img) # (1, 384, 16, 16, 16)\n",
    "\n",
    "        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "                points = [pts, labs],\n",
    "                boxes = None,\n",
    "                masks = low_res_mask.to(self.device),\n",
    "            )\n",
    "        \n",
    "        ## Decode\n",
    "        mask_out, _ = self.model.mask_decoder(\n",
    "            image_embeddings = image_embedding.to(self.device), # (B, 384, 64, 64, 64)\n",
    "            image_pe = self.model.prompt_encoder.get_dense_pe(), # (1, 384, 64, 64, 64)\n",
    "            sparse_prompt_embeddings = sparse_embeddings, # (B, 2, 384)\n",
    "            dense_prompt_embeddings = dense_embeddings, # (B, 384, 64, 64, 64)\n",
    "            multimask_output = False,\n",
    "            )\n",
    "        \n",
    "        # If repeating inference on same image, store the mask and the embedding\n",
    "        \n",
    "        if store_inference_data == True:\n",
    "            self.prev_mask = mask_out\n",
    "            self.image_embedding = image_embedding\n",
    "        \n",
    "        logits = F.interpolate(mask_out, size=img.shape[-3:], mode = 'trilinear', align_corners = False).detach().cpu().squeeze()\n",
    "        segmentation = self.postprocess_logits(logits, self.crop_params, self.pad_params)\n",
    "\n",
    "        return(segmentation)\n",
    "    \n",
    "    def clear_storage(self):\n",
    "        self.prev_mask = None\n",
    "        self.image_embedding = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/t722s/Desktop/Datasets/BratsMini/imagesTs/BraTS2021_01646.nii.gz'\n",
    "label_path = '/home/t722s/Desktop/Datasets/BratsMini/labelsTs/BraTS2021_01646.nii.gz'\n",
    "prompts_path = '/home/t722s/Desktop/Sam-Med3DTest/BratsMini/prompts.pkl'\n",
    "metadata_path = '/home/t722s/Desktop/Datasets/BratsMini/dataset.json'\n",
    "\n",
    "with open(prompts_path, 'rb') as f:\n",
    "    prompts_dict = pickle.load(f)\n",
    "\n",
    "with open(metadata_path, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "img_name = os.path.basename(img_path)\n",
    "img = sitk.GetArrayFromImage(sitk.ReadImage(img_path)).transpose(2,1,0)\n",
    "gt = sitk.GetArrayFromImage(sitk.ReadImage(label_path))\n",
    "\n",
    "\n",
    "pts = prompts_dict[img_name][1]['3D']\n",
    "pts_prompt = Points({'coords': pts[0][:,::-1], 'labels': pts[1]})\n",
    "segmenter = SAMMed3DSegmenter(sam_model_tune, (128,128,128))\n",
    "# segmentation = segmenter.predict(img, pts_prompt, store_inference_data=False, use_prev_mask=False, use_prev_embedding=False)\n",
    "# compute_dice(segmentation, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storing\n",
      "0.7855765042623359\n"
     ]
    }
   ],
   "source": [
    "prompts = []\n",
    "for i in range(5):\n",
    "    prompts.append(Points({'coords': pts[0][i:i+1,::-1], 'labels': pts[1][i:i+1]}))\n",
    "\n",
    "segmenter.clear_storage()\n",
    "segmentations = []\n",
    "segmentation = segmenter.predict(img, prompts[0], store_inference_data=False, use_prev_mask=False, use_prev_embedding=False)\n",
    "print(compute_dice(segmentation, gt))\n",
    "segmentations.append(segmentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0,5):\n",
    "    segmentation = segmenter.predict(img, prompts[:i], store_inference_data=False, use_prev_mask=False, use_prev_embedding=False)\n",
    "    print(compute_dice(segmentation, gt))\n",
    "    segmentations.append(segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/t722s/Desktop/test/embed1.pkl', 'rb') as f:\n",
    "    t1 = pickle.load(f)\n",
    "\n",
    "with open('/home/t722s/Desktop/test/embed2.pkl', 'rb') as f:\n",
    "    t2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  0,  7,  0,  0, 24])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmenter.pad_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_pad_center\n",
    "pad_params, crop_params = get_crop_pad_params(img, crop_pad_center, (128,128,128))\n",
    "\n",
    "img2 = crop_im(img, crop_params) \n",
    "img2 = pad_im(img2, pad_params)\n",
    "\n",
    "mask = img2 > 0\n",
    "mean, std = img2[mask].mean(), img2[mask].std()\n",
    "# standardize_func = tio.ZNormalization(masking_method=lambda x: x > 0)\n",
    "# img2 = np.array(standardize_func(torch.from_numpy(img).unsqueeze(0))).squeeze(0) # Gives a different result; investigate\n",
    "img2 = (img2-mean)/std \n",
    "\n",
    "img2 = torch.from_numpy(img2).unsqueeze(0).unsqueeze(0).to(device) # add channel and batch dimensions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universalModels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
