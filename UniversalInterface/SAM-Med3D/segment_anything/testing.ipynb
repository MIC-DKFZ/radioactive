{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling import ImageEncoderViT, MaskDecoder, PromptEncoder, Sam, TwoWayTransformer\n",
    "from functools import partial\n",
    "import torch\n",
    "encoder_embed_dim=768\n",
    "encoder_depth=12\n",
    "encoder_num_heads=12\n",
    "encoder_global_attn_indexes=[2, 5, 8, 11]\n",
    "image_size=256\n",
    "checkpoint='/home/t722s/Desktop/UniversalModels/TrainedModels/sam-med2d_b.pth'\n",
    "\n",
    "prompt_embed_dim = 256\n",
    "image_size = image_size\n",
    "vit_patch_size = 16\n",
    "image_embedding_size = image_size // vit_patch_size\n",
    "sam = Sam(\n",
    "    image_encoder=ImageEncoderViT(\n",
    "        depth=encoder_depth,\n",
    "        embed_dim=encoder_embed_dim,\n",
    "        img_size=image_size,\n",
    "        mlp_ratio=4,\n",
    "        norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "        num_heads=encoder_num_heads,\n",
    "        patch_size=vit_patch_size,\n",
    "        qkv_bias=True,\n",
    "        use_rel_pos = True,\n",
    "        global_attn_indexes=encoder_global_attn_indexes,\n",
    "        window_size=14,\n",
    "        out_chans=prompt_embed_dim,\n",
    "    ),\n",
    "    prompt_encoder=PromptEncoder(\n",
    "        embed_dim=prompt_embed_dim,\n",
    "        image_embedding_size=(image_embedding_size, image_embedding_size),\n",
    "        input_image_size=(image_size, image_size),\n",
    "        mask_in_chans=16,\n",
    "    ),\n",
    "    mask_decoder=MaskDecoder(\n",
    "        num_multimask_outputs=3,\n",
    "        transformer=TwoWayTransformer(\n",
    "            depth=2,\n",
    "            embedding_dim=prompt_embed_dim,\n",
    "            mlp_dim=2048,\n",
    "            num_heads=8,\n",
    "        ),\n",
    "        transformer_dim=prompt_embed_dim,\n",
    "        iou_head_depth=3,\n",
    "        iou_head_hidden_dim=256,\n",
    "    ),\n",
    "    pixel_mean=[123.675, 116.28, 103.53],\n",
    "    pixel_std=[58.395, 57.12, 57.375],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(checkpoint, \"rb\") as f:\n",
    "    state_dict = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_dict = sam.state_dict()\n",
    "except_keys = ['mask_tokens', 'output_hypernetworks_mlps', 'iou_prediction_head']\n",
    "new_state_dict = {k: v for k, v in state_dict.items() if\n",
    "                    k in sam_dict.keys() and except_keys[0] not in k and except_keys[1] not in k and except_keys[2] not in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = {k: v for k, v in state_dict['model'].items() if\n",
    "                      k in sam_dict.keys() and except_keys[0] not in k and except_keys[1] not in k and except_keys[2] not in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from(sam, state_dicts, image_size, vit_patch_size):\n",
    "    sam_dict = sam.state_dict['model']()\n",
    "    except_keys = ['mask_tokens', 'output_hypernetworks_mlps', 'iou_prediction_head']\n",
    "    new_state_dict = {k: v for k, v in state_dicts.items() if\n",
    "                      k in sam_dict.keys() and except_keys[0] not in k and except_keys[1] not in k and except_keys[2] not in k}\n",
    "    print(new_state_dict)\n",
    "    pos_embed = new_state_dict['image_encoder.pos_embed']\n",
    "    token_size = int(image_size // vit_patch_size)\n",
    "    if pos_embed.shape[1] != token_size:\n",
    "        # resize pos embedding, which may sacrifice the performance, but I have no better idea\n",
    "        pos_embed = pos_embed.permute(0, 3, 1, 2)  # [b, c, h, w]\n",
    "        pos_embed = F.interpolate(pos_embed, (token_size, token_size), mode='bilinear', align_corners=False)\n",
    "        pos_embed = pos_embed.permute(0, 2, 3, 1)  # [b, h, w, c]\n",
    "        new_state_dict['image_encoder.pos_embed'] = pos_embed\n",
    "        rel_pos_keys = [k for k in sam_dict.keys() if 'rel_pos' in k]\n",
    "\n",
    "        global_rel_pos_keys = [k for k in rel_pos_keys if \n",
    "                                                        '2' in k or \n",
    "                                                        '5' in k or \n",
    "                                                        '7' in k or \n",
    "                                                        '8' in k or \n",
    "                                                        '11' in k or \n",
    "                                                        '13' in k or\n",
    "                                                        '15' in k or \n",
    "                                                        '23' in k or \n",
    "                                                        '31' in k] \n",
    "        # print(sam_dict)\n",
    "        for k in global_rel_pos_keys:\n",
    "            h_check, w_check = sam_dict[k].shape\n",
    "            rel_pos_params = new_state_dict[k]\n",
    "            h, w = rel_pos_params.shape\n",
    "            rel_pos_params = rel_pos_params.unsqueeze(0).unsqueeze(0)\n",
    "            if h != h_check or w != w_check:\n",
    "                rel_pos_params = F.interpolate(rel_pos_params, (h_check, w_check), mode='bilinear', align_corners=False)\n",
    "\n",
    "            new_state_dict[k] = rel_pos_params[0, 0, ...]\n",
    "\n",
    "    sam_dict.update(new_state_dict)\n",
    "    return sam_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam-med",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
