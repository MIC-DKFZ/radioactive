{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "join = os.path.join\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "from segment_anything.build_sam3D import sam_model_registry3D\n",
    "from segment_anything.utils.transforms3D import ResizeLongestSide3D\n",
    "from segment_anything import sam_model_registry\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import SimpleITK as sitk\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchio as tio\n",
    "import numpy as np\n",
    "from collections import OrderedDict, defaultdict\n",
    "import json\n",
    "import pickle\n",
    "from utils.click_method import get_next_click3D_torch_ritm, get_next_click3D_torch_2\n",
    "import utils.data_loader \n",
    "import einops as E\n",
    "import torch\n",
    "Dataset_Union_ALL_Val = utils.data_loader.Dataset_Union_ALL_Val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hardcode for juptyer notebook compatability\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    test_data_path = '/home/t722s/Desktop/Datasets/BratsTestData/',\n",
    "    checkpoint_path=  '',\n",
    "    vis_path = '/home/t722s/Desktop/Sam-Med3DTest/segmentation_maps',\n",
    "    eval_dir='/home/t722s/Desktop/Sam-Med3DTest/evalBrats/',\n",
    "    interactive=True,\n",
    "    model='SAM-Med3D',\n",
    "    device = 'cuda',\n",
    "    num_clicks = 5,\n",
    "    seed=2023,\n",
    "    dim = 3,\n",
    "    crop_size = 128,\n",
    "    data_type = 'Ts',\n",
    "    split_num = 1,\n",
    "    split_idx = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model_predict3D(img3D, gt3D, sam_model_tune, device='cuda', interactive=True, points_list = None, num_clicks=5, prev_masks=None):\n",
    "    norm_transform = tio.ZNormalization(masking_method=lambda x: x > 0)\n",
    "    img3D = norm_transform(img3D.squeeze(dim=1)) # (N, C, W, H, D)\n",
    "    img3D = img3D.unsqueeze(dim=1)\n",
    "\n",
    "    click_points = []\n",
    "    click_labels = []\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    iou_list = []\n",
    "    dice_list = []\n",
    "    if prev_masks is None:\n",
    "        prev_masks = torch.zeros_like(gt3D).to(device)\n",
    "    low_res_masks = F.interpolate(prev_masks.float(), size=(args.crop_size//4,args.crop_size//4,args.crop_size//4))\n",
    "\n",
    "    for num_click in range(num_clicks):\n",
    "        with torch.no_grad():\n",
    "            # Modified by Tim\n",
    "            if(True): #(num_click==0 | interactive == False):\n",
    "                batch_points, batch_labels = points_list[num_click], torch.tensor([1])\n",
    "                print(f'Testing: points_list: {points_list}')\n",
    "            #else:\n",
    "            batch_points, batch_labels = get_next_click3D_torch_ritm(prev_masks.to(device), gt3D.to(device))\n",
    "            print(f'Testing click_method: {batch_points, batch_labels}')\n",
    "\n",
    "                \n",
    "            points_co = torch.cat(batch_points, dim=0).to(device)  \n",
    "            points_la = torch.cat(batch_labels, dim=0).to(device)  \n",
    "\n",
    "            click_points.append(points_co)\n",
    "            click_labels.append(points_la)\n",
    "\n",
    "            points_input = points_co\n",
    "            labels_input = points_la\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(utils.data_loader)\n",
    "Dataset_Union_ALL_Val = utils.data_loader.Dataset_Union_ALL_Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f():\n",
    "    # Added by Tim\n",
    "    points_list = np.array([[1],[2],[3]])\n",
    "    print(f'Before import numpy: {points_list}')\n",
    "    print(f'Before import torch: {torch.tensor(points_list)}')\n",
    "    points_mask = np.zeros(shape = (tio_image.shape[3], tio_image.shape[2], tio_image.shape[1])) # Skip the color channel for now; reintroduce in label_map definition. Reverse order since sitk uses WHD while numpy uses DHW \n",
    "    points_mask[*points_list.T] = 1\n",
    "    points_mask = E.rearrange(points_mask, pattern = 'x y z -> z y x') # Rearrange back to sitk WHD\n",
    "    points_mask = tio.LabelMap(tensor = torch.from_numpy(points_mask).float().unsqueeze(0), affine = tio_image.affine)\n",
    "\n",
    "    subject = tio.Subject(\n",
    "        image = tio_image,\n",
    "        points_mask = points_mask,\n",
    "        label = tio.LabelMap.from_sitk(sitk_label)\n",
    "    )\n",
    "\n",
    "    if '/ct_' in self.image_paths[index]:\n",
    "        subject = tio.Clamp(-1000,1000)(subject)\n",
    "\n",
    "    if self.transform:\n",
    "        try:\n",
    "            subject = self.transform(subject)\n",
    "        except:\n",
    "            print(self.image_paths[index])\n",
    "    \n",
    "    points_list = 0\n",
    "\n",
    "    return subject.image.data.clone().detach(), subject.label.data.clone().detach(), torch.tensor(points_list), self.image_paths[index] # Later don't return label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before import numpy: [[73 75 67]\n",
      " [61 46 71]\n",
      " [59 47 72]\n",
      " [54 56 67]\n",
      " [69 62 54]]\n",
      "Before import torch: tensor([[73, 75, 67],\n",
      "        [61, 46, 71],\n",
      "        [59, 47, 72],\n",
      "        [54, 56, 67],\n",
      "        [69, 62, 54]])\n",
      "After import: tensor([0])\n",
      "Testing: points_list: tensor([0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing click_method: ([tensor([[[69, 58, 71]]])], [tensor([[1]])])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ERROR] wrong size\u001b[39m\u001b[38;5;124m\"\u001b[39m, sz, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor\u001b[39m\u001b[38;5;124m\"\u001b[39m, img_name)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(args\u001b[38;5;241m.\u001b[39mdim\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m     seg_mask_list, points, labels, iou_list, dice_list \u001b[38;5;241m=\u001b[39m \u001b[43mfinetune_model_predict3D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage3D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt3D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msam_model_tune\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43minteractive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteractive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpoints_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clicks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_clicks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprev_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 21\u001b[0m, in \u001b[0;36mfinetune_model_predict3D\u001b[0;34m(img3D, gt3D, sam_model_tune, device, interactive, points_list, num_clicks, prev_masks)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Modified by Tim\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m): \u001b[38;5;66;03m#(num_click==0 | interactive == False):\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m         batch_points, batch_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpoints_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_click\u001b[49m\u001b[43m]\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting: points_list: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpoints_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m#else:\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "\n",
    "infer_transform = [\n",
    "    tio.ToCanonical(),\n",
    "    tio.CropOrPad(mask_name='points_mask', target_shape=(args.crop_size,args.crop_size,args.crop_size)), # Will center the cropping/padding at the center of the bounding box of the clicks supplied.\n",
    "]\n",
    "\n",
    "test_dataset = Dataset_Union_ALL_Val(\n",
    "    paths=args.test_data_path, \n",
    "    points_path=os.path.join(args.eval_dir, 'prompts.pkl'),\n",
    "    dim=args.dim,\n",
    "    mode=\"Val\", \n",
    "    data_type=args.data_type, \n",
    "    transform=tio.Compose(infer_transform),\n",
    "    split_num=args.split_num,\n",
    "    split_idx=args.split_idx,\n",
    "    pcc=False,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    sampler=None,\n",
    "    batch_size=1, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Load in model\n",
    "checkpoint_path = args.checkpoint_path\n",
    "\n",
    "device = args.device\n",
    "print(\"device:\", device)\n",
    "\n",
    "\n",
    "# Initialise results storage variables and directories\n",
    "out_dice_all = OrderedDict()\n",
    "\n",
    "vis_root = args.vis_path\n",
    "os.makedirs(vis_root, exist_ok=True)\n",
    "\n",
    "# Perform inference\n",
    "for batch_data in tqdm(test_dataloader):\n",
    "    image3D, gt3D, points_list, img_name = batch_data\n",
    "    print(f'After import: {points_list}')\n",
    "    sz = image3D.size()\n",
    "    if(sz[2]<args.crop_size or sz[3]<args.crop_size or sz[4]<args.crop_size):\n",
    "        print(\"[ERROR] wrong size\", sz, \"for\", img_name)\n",
    "    \n",
    "    if(args.dim==3):\n",
    "        seg_mask_list, points, labels, iou_list, dice_list = finetune_model_predict3D(\n",
    "            image3D, gt3D, sam_model_tune = None, device=device, \n",
    "            interactive=args.interactive, points_list = points_list, num_clicks=args.num_clicks, \n",
    "            prev_masks=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universalModels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
