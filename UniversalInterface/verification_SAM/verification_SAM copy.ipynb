{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nye0/SAM-Med2D/blob/main/predictor_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e574ca7",
      "metadata": {},
      "source": [
        "# SAM Comparison\n",
        "This notebook contains a stripped down version of the predictor_example.ipynb file from the SAM repo as well as code to implement my model and thus verify that they perform identically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e8492548",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change path to be one folder up to permit appropriate imports\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.path.abspath('..'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea65409b",
      "metadata": {},
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "69b28288",
      "metadata": {
        "id": "69b28288"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "    \n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
        "    \n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67418df1",
      "metadata": {},
      "source": [
        "## Compare point-prompt based segmentations\n",
        "### Use extracted code from predictor_example.ipynb to generate a segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "08aaafad",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example image\n",
        "image = cv2.imread('images/truck.jpg')\n",
        "\n",
        "# First, load the SAM model and predictor.\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "\n",
        "sam_checkpoint = '/home/t722s/Desktop/UniversalModels/TrainedModels/sam_vit_h_4b8939.pth'\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "# Process the image to produce an image embedding by calling `SamPredictor.set_image`. `SamPredictor` remembers this embedding and will use it for subsequent mask prediction.\n",
        "predictor.set_image(image)\n",
        "\n",
        "# To select the truck, choose a point on it. Points are input to the model in (x,y) format and come with labels 1 (foreground point) or 0 (background point). Multiple points can be input; here we use only one. The chosen point will be shown as a star on the image.\n",
        "input_point = np.array([[500, 375]])\n",
        "input_label = np.array([1])\n",
        "\n",
        "# Predict with `SamPredictor.predict`. The model returns masks, quality predictions for those masks, and low resolution mask logits that can be passed to the next iteration of prediction.\n",
        "masks, scores, logits = predictor.predict(\n",
        "    point_coords=input_point,\n",
        "    point_labels=input_label,\n",
        "    multimask_output=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60f6168e",
      "metadata": {},
      "source": [
        "### Now generate the same segmentation using code from this package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7948f44a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7948f44a",
        "outputId": "25d0b67a-8fd9-4140-9f98-3235caad04db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Performing inference on slices: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arrays equal? False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from classes.SAMClass import SAMInferer\n",
        "from utils.base_classes import Points\n",
        "\n",
        "# Load Model\n",
        "device = 'cuda'\n",
        "checkpoint_path = '/home/t722s/Desktop/UniversalModels/TrainedModels/sam_vit_h_4b8939.pth'\n",
        "inferer = SAMInferer(checkpoint_path, device)\n",
        "\n",
        "# My model takes 3d grayscale images, so take the image, remove the color dimension and give it a z dimension\n",
        "img_path = 'images/truck.jpg'\n",
        "img_2d = cv2.imread(img_path)\n",
        "img_3d = img_2d[:,:,0][None]\n",
        "\n",
        "\n",
        "# Obtain same prompt as in demo\n",
        "input_point = np.array([500, 375])\n",
        "input_point_3d = np.concatenate([input_point, [0]]) # include a z dimension for the point\n",
        "input_point_3d = input_point_3d[::-1] # Reverse order so xyz->zyx\n",
        "input_point_3d = input_point_3d[None] # Give N dimension\n",
        "input_label = np.array([1])\n",
        "\n",
        "prompt = Points(coords = input_point_3d, labels = input_label)\n",
        "\n",
        "# Segment\n",
        "segmentation = inferer.predict(img_3d, prompt)\n",
        "\n",
        "# Convert back to 2d to compare with original code and verify equality\n",
        "seg_2d = segmentation[0] # Select slice for z=0\n",
        "\n",
        "seg_original = masks[0] # Select first mask (only one mask returned in this case; look at comment)\n",
        "\n",
        "print(f'Arrays equal? {np.array_equal(seg_2d, seg_original)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "92ee354a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2067"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.count_nonzero(seg_2d != seg_original)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e2d5a9",
      "metadata": {
        "id": "41e2d5a9"
      },
      "source": [
        "## Compare boxes\n",
        "### From predictor_example.ipynb:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8ea92a7b",
      "metadata": {
        "id": "8ea92a7b"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread('data_demo/images/s0114_111.png')\n",
        "predictor.set_image(image)\n",
        "input_box = np.array([89,43,113,64]) #\n",
        "\n",
        "masks, _, _ = predictor.predict(\n",
        "    point_coords=None,\n",
        "    point_labels=None,\n",
        "    box=input_box,\n",
        "    multimask_output=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97b1417f",
      "metadata": {},
      "source": [
        "## From this package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c2d3a357",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using previously generated image embeddings\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Performing inference on slices: 100%|██████████| 1/1 [00:00<00:00, 25.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arrays equal? True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from utils.base_classes import Boxes\n",
        "\n",
        "# Make image 3d grayscale\n",
        "img_path = \"data_demo/images/s0114_111.png\"\n",
        "img_2d = cv2.imread(img_path)\n",
        "img_3d = img_2d[:,:,0][None]\n",
        "\n",
        "# Obtain prompt\n",
        "input_box = np.array([89,43,113,64])\n",
        "prompt = Boxes({0:input_box})\n",
        "\n",
        "# Segment\n",
        "segmentation = inferer.predict(img_3d, prompt)\n",
        "\n",
        "## Convert back to 2d to compare with original code and verify equality\n",
        "seg_2d = segmentation[0]\n",
        "seg_original = masks[0]\n",
        "\n",
        "print(f'Arrays equal? {np.array_equal(seg_2d, seg_original)}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
